{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Analysis\n",
    "\n",
    " - Part 1: Pre-processing\n",
    " - Part 2: Feature engineering (spelling errors)\n",
    " - Part 3: Predict default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42538/42538 [00:02<00:00, 16049.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lending Club data set from 2007-2011 have free-form text description, with total number of loans:  42538\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"LoanStats3a.csv\", skiprows=[0])\n",
    "# Clean the preface of \"borrower added on xx date\"\n",
    "df['desc_clean'] = df['desc'].str.replace(r\"Borrower added on \\d+\\/\\d+\\/\\d+ > \", \"\")\n",
    "\n",
    "# Remove HTML tags\n",
    "df['desc_clean'] = [re.sub(r\"<[a-z]+>\", \"\", str(desc)) for desc in df['desc_clean']]\n",
    "df['desc_clean'] = [desc.replace(\"<br/>\", \" \").strip() for desc in df['desc_clean']]\n",
    "\n",
    "# Remove email addresses and webpages\n",
    "clean_descs = []\n",
    "for i, desc in enumerate(tqdm(df['desc_clean'])):\n",
    "    # Go back to the text we originally had taken typos from        \n",
    "    n = 0\n",
    "    desc = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\", 'e-mail', desc) #Email addresses\n",
    "    desc = re.sub(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", 'webpage', desc) #webpages\n",
    "    desc = re.sub(r\"([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:s~+#-]*[\\w@?^=%&/~+#-])?\", 'webpage', desc) #webpages\n",
    "    clean_descs.append(desc)\n",
    "df['desc_clean'] = clean_descs\n",
    "\n",
    "# remove columns with all NA\n",
    "df = df.dropna(how = 'all', axis='columns')\n",
    "\n",
    "print(\"The Lending Club data set from 2007-2011 have free-form text description, with total number of loans: \", len(df['desc_clean']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default rate is  15.12 %\n"
     ]
    }
   ],
   "source": [
    "# Charged off: the loan has been in Default for 30 days or more and no more future payments are expected\n",
    "#\"Does notemeet credit policy\" is irrelevant: https://forum.lendacademy.com/?topic=2427.msg20813#msg20813\n",
    "df.loc[df['loan_status'] == \"Does not meet the credit policy. Status:Fully Paid\", 'loan_status'] = 'Fully Paid'\n",
    "df.loc[df['loan_status'] == \"Does not meet the credit policy. Status:Charged Off\", 'loan_status'] = 'Charged Off'\n",
    "\n",
    "n = df['loan_status'].value_counts()\n",
    "print(\"Default rate is \", np.round(n[1]/(n[1]+n[0])*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of loan applications with a description is  99.47 %\n",
      "Default rate for those with a description is  15.13 %\n",
      "Default rate for those without a description is  12.44 %\n"
     ]
    }
   ],
   "source": [
    "# Get the number of words in a description\n",
    "df['words_desc'] = df['desc_clean'].str.split().str.len()\n",
    "df['has_words'] = np.where(df['words_desc'] > 0, 1, 0)\n",
    "df['has_words'].value_counts()\n",
    "n = df['has_words'].value_counts()\n",
    "print(\"% of loan applications with a description is \", np.round(n[1]/(n[1]+n[0])*100, 2), \"%\")\n",
    "\n",
    "# compare default for those with and those without a description\n",
    "sub = df.loc[df['words_desc'] > 0]\n",
    "n = sub['loan_status'].value_counts()\n",
    "print(\"Default rate for those with a description is \", np.round(n[1]/(n[1]+n[0])*100, 2), \"%\")\n",
    "\n",
    "sub = df.loc[df['words_desc'] == 0]\n",
    "n = sub['loan_status'].value_counts()\n",
    "print(\"Default rate for those without a description is \", np.round(n[1]/(n[1]+n[0])*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid     35906\n",
       "Charged Off     6403\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For now, remove those without free-form comments\n",
    "df = df.loc[df['desc_clean'].notnull()]\n",
    "df = df.loc[df['words_desc'] > 0]\n",
    "\n",
    "# found one entry in Spanish - remove\n",
    "df = df[df['desc_clean'] != \"Hola, gracias por el prestamo, no les voy a fallar en ningun pago. Espero que los 206,97 USD sigan sin aumentar hasta terminar mi pago total de 36 cuotas de 206,97 USD, thank you.\"] # Remove this one entry\n",
    "\n",
    "# Get length\n",
    "df['len_desc'] = df['desc_clean'].str.len()\n",
    "\n",
    "df = df[~df['loan_status'].isnull()] #3 null loan status\n",
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.groupby('loan_status')['words_desc', 'len_desc'].describe().reset_index()\n",
    "temp.columns = [' '.join(col).strip() for col in temp.columns.values]\n",
    "temp = temp.drop([\"len_desc count\", \"words_desc 25%\",\"words_desc 75%\",\"words_desc min\", \"words_desc max\",\"len_desc 25%\",\"len_desc 75%\",\"len_desc min\",\"len_desc max\"], axis = 1)\n",
    "temp.columns = ['Loan Status', 'N', 'Mean: # words', 'Std dev: # words', 'Median: # words', 'Mean: # characters', 'Std dev: # characters', 'Median: # characters']\n",
    "temp = np.round(temp, decimals=1)\n",
    "temp['N'] = temp['N'].astype('int')\n",
    "temp['N'] = temp['N'].apply('{:,}'.format)\n",
    "temp.index = temp['Loan Status']\n",
    "temp = temp.drop('Loan Status', axis = 1)\n",
    "latex_output = temp.to_latex()\n",
    "with open(\"summary_stats_length.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_output)\n",
    "\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "\n",
    "temp = df.groupby('loan_status')['loan_amnt', 'annual_inc'].describe().reset_index()\n",
    "temp.columns = [' '.join(col).strip() for col in temp.columns.values]\n",
    "temp = temp.drop([\"annual_inc count\", \"loan_amnt 25%\",\"loan_amnt 75%\",\"loan_amnt min\", \"loan_amnt max\",\"annual_inc 25%\",\"annual_inc 75%\",\"annual_inc min\",\"annual_inc max\"], axis = 1)\n",
    "temp.columns = ['Loan Status', 'N', 'Mean: Loan Amount', 'Std dev: Loan Amount', 'Median: Loan Amount', 'Mean: Income', 'Std dev: Income', 'Median: Income']\n",
    "temp = np.round(temp, decimals=1)\n",
    "temp.index = temp['Loan Status']\n",
    "temp = temp.drop('Loan Status', axis = 1)\n",
    "temp['N'] = temp['N'].astype('int')\n",
    "temp['N'] = temp['N'].apply('{:,}'.format)\n",
    "\n",
    "latex_output = temp.to_latex()\n",
    "with open(\"summary_stats_loan_income.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset IDs\n",
    "df['id'] = range(0, df.shape[0])\n",
    "# Remove columns about outcome (e.g. interest rate offered, grade, lateness, recoveries, debt settlements, etc.)\n",
    "keep = ['id', 'loan_amnt', 'annual_inc', 'emp_length', 'home_ownership', 'loan_status', 'desc', 'purpose', 'zip_code', 'addr_state', 'desc_clean', 'words_desc', 'has_words', 'len_desc']\n",
    "df = df[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.421451\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.008</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>    <td>loan_status</td>         <td>AIC:</td>        <td>35664.9833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-22 11:30</td>       <td>BIC:</td>        <td>35690.9413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>42305</td>       <td>Log-Likelihood:</td>    <td>-17829.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>2</td>            <td>LL-Null:</td>        <td>-17982.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>42302</td>        <td>LLR p-value:</td>    <td>7.7995e-67</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>         <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>     <td>-1.6734</td>  <td>0.0297</td>  <td>-56.3143</td> <td>0.0000</td> <td>-1.7316</td> <td>-1.6151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Loan amount</th>   <td>0.0000</td>   <td>0.0000</td>   <td>14.3088</td> <td>0.0000</td> <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Annual income</th> <td>-0.0000</td>  <td>0.0000</td>  <td>-13.5691</td> <td>0.0000</td> <td>-0.0000</td> <td>-0.0000</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                         Results: Logit\n",
       "=================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.008     \n",
       "Dependent Variable: loan_status      AIC:              35664.9833\n",
       "Date:               2020-04-22 11:30 BIC:              35690.9413\n",
       "No. Observations:   42305            Log-Likelihood:   -17829.   \n",
       "Df Model:           2                LL-Null:          -17982.   \n",
       "Df Residuals:       42302            LLR p-value:      7.7995e-67\n",
       "Converged:          1.0000           Scale:            1.0000    \n",
       "No. Iterations:     6.0000                                       \n",
       "-----------------------------------------------------------------\n",
       "                  Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
       "-----------------------------------------------------------------\n",
       "Intercept        -1.6734   0.0297 -56.3143 0.0000 -1.7316 -1.6151\n",
       "Loan amount       0.0000   0.0000  14.3088 0.0000  0.0000  0.0000\n",
       "Annual income    -0.0000   0.0000 -13.5691 0.0000 -0.0000 -0.0000\n",
       "=================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "from patsy import dmatrices\n",
    "df['loan_status'] = df['loan_status'].map({'Charged Off':1, 'Fully Paid':0})\n",
    "# Split into features and labels\n",
    "y, X = dmatrices(\"loan_status ~ loan_amnt + annual_inc\", data=df, return_type='dataframe') #+ C(home_ownership,Treatment(reference='RENT')) + C(purpose,Treatment(reference='debt_consolidation'))\n",
    "X.columns = [\"Intercept\", \"Loan amount\", \"Annual income\"] #\"Home ownership: mortgage\", \"Home ownership: none\", \"Home ownership: other\",\"Home ownership: own\", \"Purpose: car\", \"Purpose: credit card\", \"Purpose: educational\", \"Purpose: home improvement\", \"Purpose: house\", \"Purpose: major purchase\", \"Purpose: medical\", \"Purpose: moving\", \"Purpose: other\", \"Purpose: renewable energy\", \"Purpose: small business\", \"Purpose: vacation\", \"Purpose: wedding\"\n",
    "# View the summary\n",
    "logit = sm.Logit(y, X)\n",
    "results = logit.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_output = results.summary2().as_latex()\n",
    "with open(\"00_base_model_Logit_LaTeX.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median income: 59000.0\n",
      "Median loan: 9750.0\n",
      "If you have median income and median loan for debt consolidation and rent your home, your probability of default is:\n",
      "15.05 %\n"
     ]
    }
   ],
   "source": [
    "from math import e\n",
    "median_income = df['annual_inc'].median()\n",
    "print(\"Median income: \" + str(median_income))\n",
    "median_loan = df['loan_amnt'].median()\n",
    "print(\"Median loan: \" + str(median_loan))\n",
    "\n",
    "# Function to return the probability, computes e^x/(1+e^x)\n",
    "coefs = results.params.values\n",
    "coefs\n",
    "\n",
    "print(\"If you have median income and median loan for debt consolidation and rent your home, your probability of default is:\")\n",
    "e_value = e**(coefs[0]+(coefs[1]*median_loan)+(coefs[2]*median_income))\n",
    "\n",
    "print(np.round(e_value/(1+e_value)*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature engineering (spelling errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify spelling mistakes using Stanford NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nONLY TO RUN FOR THE FIRST TIME - SKIP IF DATA SAVED\\n\\nimport enchant\\nd = enchant.Dict(\"en_US\")\\nfrom StanfordNER import *\\n\\n# Uses StanfordNER code to find named entities\\nset(list(Organisation(NER(\\'MasterCard, LendingClub, Citi, REO, APR, PNC, HIPPA, wna are some examples of named entities\\'))))\\n\\nfrom tqdm import tqdm\\nn_typos = []\\ntypos = []\\nclean_descs = []\\ntypos_list = []\\nrec = []\\n\\n# Look for typos\\nfor i, desc in enumerate(tqdm(df[\\'desc_clean\\'])):\\n    n = 0\\n    desc = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}\", \\'\\', desc) #Email addresses\\n    desc = re.sub(r\"(http|ftp|https)://([\\\\w_-]+(?:(?:\\\\.[\\\\w_-]+)+))([\\\\w.,@?^=%&:/~+#-]*[\\\\w@?^=%&/~+#-])?\", \\'\\', desc) #webpages\\n    desc = re.sub(r\"([\\\\w_-]+(?:(?:\\\\.[\\\\w_-]+)+))([\\\\w.,@?^=%&:/~+#-]*[\\\\w@?^=%&/~+#-])?\", \\'\\', desc) #webpages\\n    desc = re.sub(r\"[^a-zA-Z\\' ]+\", \\' \\', desc)\\n    typos_tmp = []\\n    #orgs = set(list(Organisation(NER(desc))))\\n    new_desc = [word for word in desc.split()]# if word not in orgs]\\n    desc_clean = \\'\\'\\n    for word in desc.split(): #in new_desc for org replacement\\n        if d.check(word) == False & d.check(word.upper()) == False:\\n            n += 1\\n            typos.append(word)\\n            if len(d.suggest(word)) > 0:\\n                rec.append(d.suggest(word)[0])\\n            else:\\n                rec.append(\"\")\\n            typos_tmp.append(word)\\n        else:\\n            desc_clean += word\\n            desc_clean += \" \"\\n    typos_list.append(typos_tmp)\\n    clean_descs.append(desc_clean)\\n    n_typos.append(n)\\n\\ndf[\\'typos\\'] = typos_list\\n\\ntyp = pd.DataFrame({\\'recommended\\': rec, \\'typo\\': typos})\\ntyp.drop_duplicates()\\n\\ntyp.to_csv(\"typos_v02.csv\", index = False)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ONLY TO RUN FOR THE FIRST TIME - SKIP IF DATA SAVED\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "from StanfordNER import *\n",
    "\n",
    "# Uses StanfordNER code to find named entities\n",
    "set(list(Organisation(NER('MasterCard, LendingClub, Citi, REO, APR, PNC, HIPPA, wna are some examples of named entities'))))\n",
    "\n",
    "from tqdm import tqdm\n",
    "n_typos = []\n",
    "typos = []\n",
    "clean_descs = []\n",
    "typos_list = []\n",
    "rec = []\n",
    "\n",
    "# Look for typos\n",
    "for i, desc in enumerate(tqdm(df['desc_clean'])):\n",
    "    n = 0\n",
    "    desc = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\", '', desc) #Email addresses\n",
    "    desc = re.sub(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", '', desc) #webpages\n",
    "    desc = re.sub(r\"([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", '', desc) #webpages\n",
    "    desc = re.sub(r\"[^a-zA-Z' ]+\", ' ', desc)\n",
    "    typos_tmp = []\n",
    "    #orgs = set(list(Organisation(NER(desc))))\n",
    "    new_desc = [word for word in desc.split()]# if word not in orgs]\n",
    "    desc_clean = ''\n",
    "    for word in desc.split(): #in new_desc for org replacement\n",
    "        if d.check(word) == False & d.check(word.upper()) == False:\n",
    "            n += 1\n",
    "            typos.append(word)\n",
    "            if len(d.suggest(word)) > 0:\n",
    "                rec.append(d.suggest(word)[0])\n",
    "            else:\n",
    "                rec.append(\"\")\n",
    "            typos_tmp.append(word)\n",
    "        else:\n",
    "            desc_clean += word\n",
    "            desc_clean += \" \"\n",
    "    typos_list.append(typos_tmp)\n",
    "    clean_descs.append(desc_clean)\n",
    "    n_typos.append(n)\n",
    "\n",
    "df['typos'] = typos_list\n",
    "\n",
    "typ = pd.DataFrame({'recommended': rec, 'typo': typos})\n",
    "typ.drop_duplicates()\n",
    "\n",
    "typ.to_csv(\"typos_v02.csv\", index = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typo</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im</td>\n",
       "      <td>mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>takingmeplaces</td>\n",
       "      <td>marketplaces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>payor</td>\n",
       "      <td>mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Downpayment</td>\n",
       "      <td>Down payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eg</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             typo   recommended\n",
       "0              im            mi\n",
       "2  takingmeplaces  marketplaces\n",
       "3           payor         mayor\n",
       "4     Downpayment  Down payment\n",
       "5              eg             g"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typ = pd.read_csv(\"typos_v02.csv\")\n",
    "columns = ['typo', 'recommended']\n",
    "typ = typ.drop_duplicates()\n",
    "typ = typ[columns]\n",
    "typ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SKIP IF ALREADY SAVED **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from StanfordNER import *\n",
    "from tqdm import tqdm\n",
    "orgs = []\n",
    "for i, typo in enumerate(tqdm(typ['typo'])):\n",
    "    if len(Organisation(NER(typo))) > 0:\n",
    "        orgs.append(typo)\n",
    "\n",
    "orgs[:10]\n",
    "\n",
    "orgs_df = pd.DataFrame({\"organisations\": orgs})\n",
    "orgs_df.to_csv(\"orgs.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique typos: 7997\n",
      "Organisations removed: 284\n",
      "Organisations removed (lowercase): 35\n"
     ]
    }
   ],
   "source": [
    "orgs_df = pd.read_csv(\"orgs.csv\")\n",
    "\n",
    "orgs = orgs_df['organisations']\n",
    "orgs[:10]\n",
    "orgs = list(orgs)\n",
    "\n",
    "len_0 = typ.shape[0]\n",
    "print(\"Unique typos: \" + str(len_0))\n",
    "typ = typ[~typ['typo'].isin(orgs)]\n",
    "len_1 = typ.shape[0]\n",
    "diff = len_0 - len_1\n",
    "print(\"Organisations removed: \" + str(diff))\n",
    "len_0 = len_1\n",
    "\n",
    "lowercase_orgs = typ[typ['typo'].isin([org.lower() for org in orgs])]\n",
    "orgs.extend(lowercase_orgs['typo'])\n",
    "typ = typ[~typ['typo'].isin([org.lower() for org in orgs])]\n",
    "len_1 = typ.shape[0]\n",
    "diff = len_0 - len_1\n",
    "print(\"Organisations removed (lowercase): \" + str(diff))\n",
    "len_0 = len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acronyms removed: 516\n"
     ]
    }
   ],
   "source": [
    "capitalised_words = [word for word in typ['typo'] if word.isupper()]\n",
    "acronyms = ['REO', 'LLC', 'GECRB', 'OLND', 'SYM', 'JC', 'DTI', 'CPVC', 'FCU', 'UC', 'FICO', 'CARDSI', 'HSA', 'HELOC', 'TDI', 'CPG', 'USD', 'FIOS', 'XJ'\n",
    "            , 'CEFCU', 'PIF', 'HVAC', 'CVS', 'ETS', 'APY', 'BP', 'WR', 'JPM', 'MH', 'DMV', 'SEO', 'HFC', 'RZR', 'CFP', 'EZ', 'YNAB', 'THB', 'DDA', 'BAE'\n",
    "            , 'PGA', 'KLR', 'NICU', 'FXR', 'ENR', 'EBAY', 'ETF', 'JWE', 'NOY', 'SAIC', 'GMC', 'GPS', 'DS', 'DMP', 'HWCU', 'NCLEX', 'DRW', 'JMH', 'HD', 'GH'\n",
    "            , 'EIN', 'IVF', 'PMI', 'PITI', 'HOA', 'RTP', 'GSF', 'WFFNB', 'WFNNB', 'TU', 'DIY', 'TGIF', 'TCU', 'ACCTS', 'ELE', 'MSC', 'PV', 'WD', 'RGN'\n",
    "            , 'LOC', 'GPW', 'AES', 'KJ', 'VNA', 'MFCU', 'SSG', 'SL', 'AWD', 'LX', 'CHP', 'FPU', 'GSXF', 'CSY', 'TRLR', 'GW', 'DM', 'DX'\n",
    "            , 'AGI', 'CALSTATE', 'FIA', 'MCU', 'VSAC', 'TUCIC', 'DVD', 'DTE', 'ACURA', 'DIL', 'GTI', 'ADME', 'RTQ', 'PMP', 'RGE', 'EER', 'LSU', 'SSN'\n",
    "            , 'CHGO', 'NHSC', 'AMD', 'SBT', 'FC', 'SSDI', 'ESPP', 'CSA', 'DMAG', 'PMXG', 'CJ', 'SLT', 'MSFCU', 'ZP', 'XB', 'NYS', 'RG', 'XR', 'NCU', 'BSN'\n",
    "            , 'EE', 'TUC', 'EFX', 'UW', 'NSX', 'ACH', 'OTI', 'WF', 'ZX', 'HDHP', 'RK', 'ALS', 'CVT', 'VTX', 'APRS', 'TUONO', 'DSL', 'GEMB', 'SSI', 'USAFR'\n",
    "            , 'YTD', 'FX','OKC','DFW','PECI','DEDT', 'SG', 'CRM', 'SDG', 'CCL', 'MRSA', 'SGM', 'BAC', 'POS', 'TMS', 'NYSDOT', 'IRP', 'KYU', 'IRB', 'MSB'\n",
    "            , 'EO', 'GL', 'BOPP', 'PTSD', 'CWIE', 'FFL', 'TTO', 'NHA', 'ARRA', 'EHR', 'BAM', 'NYU', 'CRC', 'PPG', 'SOA', 'FTASAP', 'FFAE'\n",
    "            , 'SER', 'RET', 'ARET', 'PIMA', 'JRV', 'CSR', 'CCSD', 'RIF', 'LANTEC', 'JK', 'VSG', 'THP', 'PHR', 'BSEE', 'EMI', 'API', 'DBA'\n",
    "            , 'ASR', 'SACU', 'APAP', 'UOP', 'GTH', 'UTC', 'EHS', 'MOASC', 'EQR', 'DSG', 'ENAGIC', 'CPT', 'SECU', 'HH', 'AIG', 'NDT', 'FBA', 'SUNUS', 'OPC'\n",
    "            , 'IPO', 'PRE', 'UNFCU', 'CNOR', 'LLP', 'AVP', 'TCABC', 'SSID', 'QCC', 'JCB','EQ', 'ING', 'SNE', 'FP',  'OPERS', 'RIAZ', 'RFID'\n",
    "            , 'XLE', 'YHJF', 'SAAS', 'ADT', 'CRX', 'CFNA', 'SSL', 'SSD', 'WCCUSD', 'MSKCC', 'DMS', 'SPX', 'ATT', 'NISCAYAH', 'ASCP', 'TMAT'\n",
    "            , 'CTS', 'AMR', 'AEP', 'USPTO', 'XLT', 'TT', 'SFPUC', 'FAULTIY', 'CCARD', 'APS', 'ASLO', 'RSU', 'ACK', 'JCP', 'REFI', 'VH', 'SHSAT'\n",
    "            , 'GRE', 'GMAT', 'NSIT', 'KSL', 'BML', 'HEL', 'GBP', 'GF', 'DAVISON', 'COFS', 'RTG', 'FMV', 'TFS', 'VC', 'DSNB', 'VSP', 'DNB', 'PAYDEX', 'DIT'\n",
    "            , 'TNB', 'MBM', 'BUIL', 'NGV', 'PTR', 'GSX', 'VPS', 'TXU', 'UPB', 'SOX', 'CCCU', 'MCSE', 'WMC', 'GFI', 'SLC'\n",
    "            , 'USF', 'MOBILEX', 'BPC', 'MLM', 'SVP', 'PIMI', 'DL', 'CMU', 'TTU', 'BAL', 'PYMT', 'GRSB', 'BD', 'BTH', 'FDNY', 'PAVERS', 'PCI'\n",
    "            , 'KTM', 'STRS', 'SQL', 'HMC', 'FASFA', 'EMG', 'EDMC', 'PMA', 'DFCU', 'LASIK', 'BYU', 'APPT', 'UNM', 'BECU', 'CFA', 'PMT'\n",
    "            , 'UAB', 'STUC', 'ISC', 'ISPC','EIP', 'FAKO', 'PPA', 'WPPI', 'CTM',  'FSA', 'OCSD', 'RO', 'EAM', 'RBM', 'FSU', 'YZF', 'ERP', 'CFE', 'RHS', 'MPA'\n",
    "            , 'PGE', 'FSH', 'TMJ', 'CONUS', 'AWI', 'VIT', 'PTZ', 'XLG', 'HDR', 'SRT', 'MRO', 'FIU', 'DECU', 'VZ', 'KCPD', 'BAYADA', 'RFP', 'DHS', 'FML','SRE'\n",
    "            , 'WSM', 'GYN', 'BELK', 'BFC', 'EG', 'FDOT', \"'KMA\",'NYSTRS', 'FLETC', 'XT', 'EMR', 'AGN', 'HELCO', 'MSOE', 'WTH', 'PLTW', 'IEA', 'TEP'\n",
    "            , 'OSU', 'KPF', 'HOK', 'LUMI', 'MSN', 'CRF', 'NOI', 'AE', 'USAR', 'KLOC', 'FSFSA', 'MRP', 'SDVOSB', 'CP', 'GSXR', 'ISP', 'ATL', 'VMI', 'COCKRELL', 'ARHCU'\n",
    "            , 'GNG', 'TALCE', 'CRE', 'IAAVS', 'PSU', 'DCR', 'GBS', 'EJ', 'GTO', 'NCT', 'PBW', 'MLB', 'GMAIL', 'MLS', 'ARMAC', 'JNKCV', 'WVO', 'CNY'\n",
    "            , 'USPCC', 'RLU', 'WIFI', 'CUNY', 'WH', 'OBX', 'POC', 'BOM', 'JVC', 'GY', 'SES', 'EDU', 'DRI', 'RSS', 'RTO', 'BHPH', 'JIC', 'QE', 'DDR'\n",
    "            , 'QSOL', 'APA', 'BT', \"YE'\", 'REI', 'SOFIJA', 'DSW', 'MCSA', 'ISU', 'CCI', 'ADR', 'PDF','CSU', 'DSLR', 'IP', 'VOC', 'RIT', 'LLM', 'AIA', 'PNW', 'BH'\n",
    "            , 'APK', 'GUSD', 'GFE', 'BNY', 'LSAT', 'LOA', 'IMS', 'DFS', 'PRM', 'ACS', 'GRT', 'PUD', 'CCS', 'NRG', 'JCL', 'QMRP'\n",
    "            , 'CCNA', 'DVT', 'BCC', 'JBL', 'ZD', 'ITN', 'AMX', 'USACII', 'USACCI', \"FY'\", 'CQF', 'SJSU', 'LVN', 'TMJD', 'LV', 'OPB', 'DTL', 'ESPINDOLA', 'UVA', 'SEP'\n",
    "            , 'SEM', 'PULISH', 'CSET', 'CBEST', 'PLS', 'GMAX', 'DFACS', 'Oll', 'APR', 'SUV', 'USAF', 'WWW', 'ATV', 'NYC', 'SCCU', 'NASDAQ', 'HTTPS', 'VRHS'\n",
    "            , 'UFCS', 'BMW', 'CPA', 'ISA', 'IRS', 'IRA', 'ATM', 'EMT', 'SSA', 'FHA', 'RSVP', 'OSHA', 'NACA', 'SAM', 'USA', 'MSA', 'MRI']\n",
    "typ = typ[~typ['typo'].isin(acronyms)]\n",
    "\n",
    "len_1 = typ.shape[0]\n",
    "diff = len_0 - len_1\n",
    "print(\"Acronyms removed: \" + str(diff))\n",
    "len_0 = len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at words starting with capital letter and remove non-typos\n",
    "capitalised_words = [word for word in typ['typo'] if word[0].isupper()]\n",
    "\n",
    "entities = ['Citi', 'LendingClub', 'Transunion', 'Experian', 'USbank', 'DiscoverCard', 'APRs', 'Bal', \"CC's\", 'LENDINGCLUB', 'STARTUPSNAX', 'NYCTRINITY', 'CBGONZALEZ' \n",
    "            , 'USBank', 'Baloon', 'Verinik', 'WARESALE', 'ZENSAH', 'NUSBAND', 'TRANSUNION', 'SUGARSAND', 'RATON', 'DADELAND', 'SHOPSTORES','WELLCRAFT', 'INGROUND', 'SURBURBAN'\n",
    "            , 'Eloan', \"Raley's\", 'Broward', 'AmEx', 'Zoso', 'MedAssets', 'Lowes', 'Jetta', 'BoA', 'Pennys', 'CREDITALS', 'Sportsmans', 'TransUnion', 'MOTORHOME'\n",
    "            , 'Happypurpose', 'Happycredit', 'Lasalle', 'Durango', 'Dodd', 'Amex', 'Springleaf', \"Francisco'\", 'BlueBook', 'Sharebuilder', 'CROSSFIT', 'USBANK', 'NEHOTDOG'\n",
    "            , 'Toysrus', \"Reserve's\", 'Samsung', 'Poolboss', 'Motorhome', 'Pepperdine', 'Refi', 'Hepa', 'LinkedIn', 'YouTube', 'GooglePlus'\n",
    "            , 'NexTier', 'Zillow', 'CreditCards', 'CCs', \"APR's\", 'BofA', 'Px', 'Lancruiser', 'CreHigh', 'Wa', 'Suze', 'Orman', 'Cabelas', 'Hodgkins'\n",
    "            , 'Macys', 'Penneys', 'Sams', 'Crownline',  'Holsteiner', 'Fauxwood', 'Telecom', 'CapOne', 'Nordstroms', 'Aprilia', 'Shovelhead', 'Soffits', 'AAdvantage'\n",
    "            , 'Supercrew', 'Levantina', 'MacBook', 'Yaris', 'Hidenwood', 'Fico', 'Ipay', 'Prostodontics', 'Endodontics', 'Pune', 'OneMain'\n",
    "            , 'Brookwood', 'Paydown', 'QuickBooks', 'ProAdvisor', 'SilverDollar', 'Airboat', 'Derebery', \"Jones's\", 'HomeDepot', 'Kawa', 'Ponte', 'Vedra'\n",
    "            ,  'Rosso', 'Elec', 'Virco', 'Moly', 'Ues', 'CCards', 'Sportster', 'Citicards', 'Citflex', 'Kenai', \"HOA's\", 'Ironmans', 'Cranbrook'\n",
    "            , 'Citicard', 'GeMoney', 'Truecredit', 'Iplan', 'Uhaul', 'MacbookPro', 'AdWords', 'AdCenter', 'Chartplotter', 'Consolidatin', 'Inground', 'Davidsons'\n",
    "            , \"DVD's\", 'NewEgg', 'Newegg', 'Depts', 'Acetylyne', 'Drupal', 'Lendingclub', 'Mgmt', \"SKU's\", 'NGOs', 'Eller', 'Waverunners', 'Freightliner', 'DeVille'\n",
    "            , 'Cyl', 'Photovoltaic', 'Enphase', 'Kickstarter', 'ConEdison', 'Ebay', 'Rolex', 'CapitalOne', 'BestBuy', 'UHaul', 'NMCCs', 'MBAs', 'Util', 'Biotech'\n",
    "            , 'Pacifica', 'Capitalone', 'Eqifax', 'Gooseneck', 'CitiCard', 'GoogleTV', 'Denali', 'Sixeas', 'Morro', 'Obispo', 'Mics', 'Lakeview', 'Silverado', 'Ducati'\n",
    "            , 'Burea', 'DoD', 'Whitworth', 'Howards', 'ATMs', 'Wi', 'Doughboy', 'Meetup', 'Groupon', 'App', \"ON'S\", 'Lufkin', 'TJMaxx', 'Stomas', 'Tubo'\n",
    "            , 'Injecter', 'Reatta', 'Ater', 'Trailmanor', 'VINEYL', \"HOUSE'\", 'AmexCard', \"EV's\", 'Halliburton', 'MegaCab', 'Cummins', 'Sienne', 'Speedmaster'\n",
    "            , 'Semper', 'Fi',  'Starcraft', 'Vespa', 'Licence', 'Citicrad', 'SallieMae', 'Aquarian', 'Carecredit', 'Barclays', 'Corrolla', 'Countertops', 'Rv'\n",
    "            , 'MyFICO', 'FLa', 'Altima', 'WiFi', 'Opry', 'Promissary', 'Maint', 'Camaro', 'AmeriCorps', \"LendingClub's\", 'Wal', 'SolarCurrents', 'Miscel'\n",
    "            , 'Online', 'Discovercard', 'Prius', 'Tks', 'Fairwinds', 'Bowflex', 'Roamans', 'Deby', 'CareCredit', 'Juris', 'Experion', 'Boomers', 'SolarCity', 'Soboba'\n",
    "            , 'Neiman', 'Acura', 'Bally', 'Pilates', 'Joes', 'Bly', 'Gunite', 'Pennsauken', 'Adwords', 'Exquifax', 'Worldmark', 'Chillicothe', 'Starscapes', 'Hardie'\n",
    "            , 'Craigslist', 'Fresenius', 'Spooner', 'Sundancer', 'Phuong', 'Sturgis', 'Kawsaki', 'Ludlum', 'Tha', 'ServiceLink', 'Zumba', 'Motorsports', 'Oneonta', 'Thany'\n",
    "            , 'BankAmericard', 'Permananete',  'Homedepot', 'Hava', 'Virto', 'Lookings', 'Jamall', 'Elmendorf', 'Nitro', 'Brammo', 'Citiibank', 'Skidoo', 'Terminix'\n",
    "            , 'Volusia', 'Pharma', 'Skidsteer', 'RealPage', 'Obama', 'Tuono', 'DVDs', 'Lunds', \"Byerly's\", 'Goldwing', 'Bic', 'Flippa', 'Pipeliner', 'Clight', 'Celica'\n",
    "            , 'WinePressGroup', 'Neuroscience', 'Bridgpoint', 'Reattas', 'Wellsfargo', 'SpeechEasy', 'Djs', 'Jaytee', 'Arabica', 'Landstar', 'Aslo', 'Medway'\n",
    "            , 'Rucker', 'Tictok', 'Cuyahoga', 'MRIs', 'Eldy', 'Fatboy', 'SteakNShake', 'Lumina', 'Razorfish', \"Chron's\", 'CPAs', 'Pharm', 'CrossFit', 'Stroudsburg', 'LendingCLub', 'Kretek'\n",
    "            , 'Arblebeedie', 'Beelte', 'Paintball', 'Blogs', 'Youtube', 'Flickr', 'QuinnipiacUniversity', 'Chula', 'Saks', 'Deltacom', 'NuVision', 'Springville'\n",
    "            , 'Clackacraft', 'Vuitton', 'Moet', 'Hennessy', 'Husqvarna', 'Cleanpro', 'Westfalia', 'Volkswagon', 'Salesforce', 'CIOs', 'Uinta', 'Menards', 'Daytona'\n",
    "            , 'Chennai', 'Changs', 'Rhis', 'Cummington', 'Ramapo', 'Rhinebeck', 'Laguardia', 'Biodot', 'Kiva', 'Minumum', 'Santander', 'Maxima', 'GMoney', 'Gmoney'\n",
    "            , 'Lukes', 'Wadell', 'Duathlon', 'DasM', 'Sno', 'Merita', 'WorldMark', 'Pre', 'Phd', 'JETBOAT', 'KonicaMinolta', 'Juilliard', 'Greatwide'\n",
    "            , 'ClearPointe', \"Crohn'\", 'Greiner', 'Duckman', 'ETFs', 'Ishares', 'Patrisia', 'GEMoney', 'JPMorgan', 'Dicover', 'Opgal', 'Villanova', 'Wamu'\n",
    "            , 'Airstream', 'WellsFargo', 'Fiserv', 'AlwaysOn', 'Vallarta', 'Alcapulco', 'Esthetician', 'Un', 'Exoress', 'PeaceHealth', 'Apnea', 'Haistar', 'Festivus'\n",
    "            , 'CreditKarma', 'Aldi', 'Buckhead', 'Regence', 'PocketSmith', 'Aliant', 'Monitronics', 'Cheernastics', 'Jarden', 'Moto', 'Az', 'ThomsonReuters'\n",
    "            , 'BelAir', 'Kinesiology', 'Neorstar', 'SearsCreditScore', 'Ameriprise', 'CashReturns', 'Demusz', 'Fios', 'Helzberg', 'Lemans', 'Darden', 'Novasure'\n",
    "            , 'Sephora', 'EurAuPair', 'Agi', 'Dvd', 'Dupage', 'Phils', 'LCs', 'HearUSA', 'Lasik', 'Hanford', 'Redbox', 'DVDNow', 'Nori', 'Mattingly', 'Nutritionals'\n",
    "            , 'Evercleanse', 'Enagic', 'Pascagoula', 'AmWINS', 'Rostami', 'LClub', 'Francetta', 'Loius', 'Branson', 'Mauldin', 'Armedia', 'Ikea', 'Tallahasse'\n",
    "            , 'Chickfila', 'Aarons', 'Soalr', 'Formostar', 'Ameritrade', 'Frontsight', 'CRv', 'Hempfield', 'Acromegaly', 'Cephalon', 'Blog', 'FriendFinder'\n",
    "            , 'Markarian', 'Wedgewood', 'Pandit', 'Keuka','Sping', 'Pers', 'Exterran', 'Hbsc', 'AutoCAD', 'CreditSecure', 'Lanaii', 'PRs', 'Haywayrd'\n",
    "            , 'Vianina', 'Raymour','Flanigan', 'Pricegrabber', 'Dorathy', 'Amblin', 'Lae', 'Gavilon', 'Rewardzone', 'Quattro', 'WordPress', \"Haverty's\", 'BjB', 'Cas'\n",
    "            , 'Paratransit', 'Kompressor', \"Oletello's\", 'Infiniti', 'MVelopes', 'Accenture', 'Puliz', 'Hibdon', 'Walla', 'Pannels', 'Councelor', 'Ridgeline'\n",
    "            , 'Loganville', 'Genco', 'Petco', 'Mifflin', 'SharePoint', 'Waverunner', 'Waverrunner', 'Xcentric', 'LexisNexis', 'Cryptologics', 'Permanente'\n",
    "            , 'Taubman', 'Fairlane', 'Spokanes', 'Raton', 'Quizzle', 'Phenom', 'Electraglide', 'Kahren', 'Bentonville', 'FirstBank', 'LLBean', 'Weisfield'\n",
    "            , 'Flitestar', \"Jones'\", 'Crd', 'Opici', 'Aetna', 'CaptialOne', 'Hazmat', 'Chevelle', \"Sear's\", 'JcPenny', 'VIsa', 'Bestbuy', 'Xbox', 'Iphone', 'Symbian'\n",
    "            , 'Bens','Mitsui', \"Kinko's\", 'TrueCredit','Alpharetta', 'MasterCards', 'Fingerhut', 'Edwardsville', 'Coldwell', 'Realogy', 'Greewood', 'Bayliner'\n",
    "            , 'Softtail', 'PreK', 'Providian','Ultrasonographer', 'Nextel', 'Usbank', 'Charlevoix', 'Petoskey', 'Mamaroneck', 'Jacksboro', 'Revoling', \"LC's\", 'HELOCs'\n",
    "            , 'Nagahama','Captil', 'Landline', 'NetJets', 'Birkshire', 'Converseo', 'Citifincial', 'Tarrant', 'REOs', 'Palmatte', 'Riteway','Yoon', 'Bon', 'CitCard','GIBill'\n",
    "            , 'Willys', 'Bikram', 'Marland','Corvair','Ratha','CareOne', 'Velux', 'Horicon', 'DIsh', 'Technition', 'Elliman', 'Wilm','Experien', 'Lotawana', 'Azusa'\n",
    "            , 'GeoThermal', 'GoeThermall', 'Tencor', 'Jayco', 'Wairarapa', 'Sauv','Bellevue', 'Harborview', 'Groveport', 'Doula','Arbonne', 'Impreza', 'Fonality', 'Xpres'\n",
    "            , 'Repulic', 'Hidra', 'Ckass', 'Phoenis', 'Dadeland', 'CriticalMiss', \"Fiancee'\", 'ExperianCredit', 'CNNMoney', 'ProSci', 'AntiCancer', \"Sleepys'\", 'MediMedia'\n",
    "            , 'Insur', \"Northwestern's\", 'Tyra', 'CCConcepts',  'Tec', 'Debutante', 'Tradewinds', 'Ipad', 'CDs', 'BerkleeMusic', 'Tourneau', 'Wellfargo', 'Sampco'\n",
    "            , 'EnerBankUSA', 'Shoprite', 'Deloitte', 'Targa', 'Montefiore', 'Taylorville', 'Bof', 'Oryon', 'Manatt', \"O'Melveny\", 'Vumii', \"PO'd\", 'Barclarys', 'Tradeshow'\n",
    "            , 'Brighthouse', 'Macbook', 'MacDill', 'Engelbreit', 'InBev', 'SuperBowl', 'Yakira', 'Lemoore', 'Danise', 'Corix', \"VP's\", 'McArdle', 'Integra', 'Brea'\n",
    "            , 'Celink', 'CreditClub', \"CD's\", 'DJs', 'Plex', 'Cama', 'Appx', 'Granbury', 'Echocardiography', 'Sqft',  'Crestron', 'Grooveshark', 'UMass', 'Stonehill'\n",
    "            , 'Shinwa', 'Victorias', 'Lovetri', 'Temecula', 'Bluewater', 'Watauga', 'Massons',  'Skydrol', 'Lan', 'Panera', 'Wipfli', 'Keiser', 'FullSail'\n",
    "            , 'UChicago', 'McNair', \"Starbuck's\", \"Children'sHospital\", 'Automall', 'Pinellas', 'VisaCard', 'Teksystems', 'Telerx', 'Eldercare','Hujiang', 'Falmouth'\n",
    "            , 'Dj', 'Roto', 'Tula', 'Viscira', 'VitesseLearning', 'Tv', 'SoHo', 'Cobham', 'DelMarVa', 'Oth',  'Sugarland', 'AADvantage', 'Terracon', \"CEO's\", 'Botteling'\n",
    "            , 'Suisse', 'CalPERS', 'Transcriptionist', 'Bloomingdales', 'Forex', 'Elan', 'Petropolis', 'Voc', 'Internt', 'SUVs', 'FairLenderUSA', 'Waikele'\n",
    "            , 'CSRs', \"Obama's\", 'Precor', \"Cummuta's\", 'Sccfcu','Aidance', 'CLub', 'INGDirect', 'PwC', 'GapCard', 'Abercrombie', 'Solopreneurs', 'Nambia', 'Voip'\n",
    "            , 'CitiFinance', 'Anderso', 'Biomed', 'Ewings', 'Oxymorons', \"Batter's\", 'McKinsey', \"MRI's\", 'Reddit', 'DeVry', 'Belvoir', 'Eldor', 'Euro'\n",
    "            , 'Delray', 'AmarOil', 'Welch', 'CableOne', 'Alagasco', 'Agrifos', 'WalletPop', 'Unifed', 'Macdill', 'Osan', 'TheraCom', 'Bofa','QuadKing', 'Arthurs'\n",
    "            , 'Awfularthursseafood', 'Kosovo', 'TruGreen', 'Quozzy', 'Grande', 'Metalcraft', 'Eisai', 'Booz', 'Kenexa', \"SRE's\", 'Conica', 'Alos', 'Zeor'\n",
    "            , 'Timberlodge', 'Flannigan', 'Utilites', 'DMGetDocument', 'LexisNexs','GetFugu', 'Ashford', 'Sealift', 'LaJolla', 'Sause', 'Rehoboth', 'TreeFarmer', 'Mactec'\n",
    "            , 'HighSpans', 'Maricopa', 'Ymca', 'Pricewaterhousecoopers', 'Femoroacetabular', 'Rubilotta', 'Copey', 'Desku', 'Rishi', 'Bhat', 'Telsmith', 'CommID'\n",
    "            , 'Yahama', 'Eagan', 'Acitve', \"Thacaro's\", 'Becon', 'Sedona', 'Poolcraft', 'Gensler', 'Poly', 'Kyomatt', 'Moviebox', \"Moviebox's\", 'Blu', 'StocksMan'\n",
    "            , 'HealthCentral', 'CashFlow', 'Ridgewood', 'Alaskian', 'Malmute', 'Inder', 'Prodigo', 'Ramsy', 'Gallaudet', 'OnSite', 'Integreon'\n",
    "            ,  'Tulum', 'Towncar', 'Blackhawks', 'Edelbrock', 'Amanti', 'Tx', 'Crohns', 'Taekwondo', 'Ridgeback', 'Learjet', 'Barryville', 'Praveen', 'BigDog', 'Menendez'\n",
    "            , 'Kamran', \"Momma's\", 'Somatics', 'VanKeuren', 'Broco', \"Rubio's\", 'Ucla', \"Rhode's\", 'Napa', 'Deskin', 'Metroplex', 'Selem', 'Sholin', 'Dass'\n",
    "            , 'Sonographer', 'Pecnik', 'Marymount', 'Ghazi', 'Swann', 'DeBrino', 'MorphBrand', 'Anahiem', 'Gerig', 'Yaworsky', \"IR's\", 'Rhode', \"Bug'\", 'Medaxial'\n",
    "            , 'DebtFreeAdventure', 'Ramen', 'Havastat', 'Aquiring', 'Loc', 'Chia', 'Lerma', 'Deepwater', 'Geschwindt', 'Renior', 'Bellingham', 'DingKing', 'Imtiaz'\n",
    "            , 'Fernie', 'Goico', \"RV's\", 'Schlenz','Bio', 'Khecari', 'Newsteps', 'Flieger', 'GoGoVertigoat', 'BoCoCa', 'Mancuso', 'Natchitoches', 'FritoLay', 'Anc'\n",
    "            , 'Canisius', 'Curesome', 'Kehoe', \"RN's\", 'Akerstein', 'Regolith', \"Aug'\", 'Niguel', 'Girbes', 'Markesia', 'Akinbami', 'Sampsel', 'Mercruiser'\n",
    "            , 'Choate', 'Paseo', 'Brzostowski', 'Centeno', 'Regenexx', 'Kamdolla', 'Collums', 'Kenwood', 'Asano', 'Taiko', 'Hono', 'Hanes',\"Alphie's\", 'NextGen', 'Alaaddin'\n",
    "            , 'Braikbrothers', 'Arborist', 'Sonography', 'Turnage', 'Garbow', 'Umpqua', 'Hatchett', 'Tysabri', 'Mudd', 'Disneycruise', 'Janovsky', 'Ponitac', 'Lendin'\n",
    "            , 'Goldbar', 'Obert', 'Edmunds', 'PowerShares', 'WilderHill', 'Jc','Materail', 'Bramwell', 'Dmitriy', 'Phy', 'Asplundh', 'TULESFROZENYOGURT', 'Glavin', 'Rosner'\n",
    "            , 'Lewallen', 'SSgt', 'Whe', 'Adhish', 'Paydex', 'Daihatsu', 'Mastergeorge', 'MacMurray', 'Tiffaney', 'LeadAmerica', 'ARMs', 'DollarServ'\n",
    "            , 'Valento', 'Bair', 'CreditEducation', 'WhatsInYourScore', 'Jumpshot', 'Tribeca', 'Merceds', 'Kona', 'Saracino', 'Twan', 'RogerGrady', 'LLCs'\n",
    "            , 'PricewaterhouseCoopers', 'Citimortgage','CDOs', 'ARMSs', 'Zurich', 'Hayabusa', 'Hsbc', 'Citifancial', 'Hodder', 'Dezendorf', 'Enfield', 'Franske', 'Petrie'\n",
    "            , 'Baij', 'McQueary','Viccaro', 'Nav', 'Kenner', 'LWLMedia', 'Banknorth', 'BioDiesel', 'Biodiesel', 'Linconl', 'SKUs', 'JEPook', 'Xb', 'Deville'\n",
    "            , 'VocRehab', 'Trooptastic', 'Appleman', 'Belfair','Grapeview', 'Kitsap', 'Bremerton', 'Oviedo', 'WebSuccessAgency', 'SaaS', 'SaaC'\n",
    "            , 'SalesForce', 'Marleny','Groza', 'RDMc', 'RDMcPublishing', 'Archicad', \"GMAT's\", 'Kubota', 'Surger', 'Locums', 'Lesabre', 'VisitorsGuide', 'NorthBeachSun'\n",
    "            , 'Lowrey', 'Petworth', 'Slattery', 'Tingen', 'Jivox','Anthropolgy', 'Blas', 'Levitz', 'NetworkPlusConsulting', 'BusinessPlanforGSITF', 'Binu'\n",
    "            , 'Eapen', 'Aviastroitel', 'Weizen', 'Shapewear', 'Bodyshaping', \"HUSH's\", 'Murtza', 'Naseem', 'Xterra', 'GMATs', 'Humana', 'BlueStarTees'\n",
    "            , 'FolioFN', 'AffiliateDiamond', 'TutoringOne', 'WebChat', 'Topolski', 'Marist', 'Bac', 'Furthersite', 'Secretarys', \"SUV's\", 'Wintersville'\n",
    "            , 'Alibaba', 'Powerseller', 'VoIP', 'StartupSnax', 'Munchables', 'Ebook', 'Ebooks', 'PublishersWeekly', 'Hazlewood', 'Sanitaire', 'Eastover', 'Maxfield'\n",
    "            , 'Nirav', 'Subrau', 'Tamie', 'Opteron', 'Psy', 'Carrollton', 'Jasniewski', 'Mcbee', 'Kripalu', 'Vero', 'Soloman', 'Mazdaspeed', 'Pallisades', 'Rehana'\n",
    "            , 'Husain', \"Orman's\", 'Stott', 'Bluenile', 'Nordstorm', 'Charnel', 'Everly', 'Newberry', 'Credet', \"SIL's\", 'DandD', 'DandDToys', \"ATV's\", 'VinyasaMT'\n",
    "            , 'Barack', 'Sawgrass', 'Banos', 'SoCal', 'Touche', 'Sempra', \"MBA's\", 'Lithia', 'Wai','Ipods', 'Nano', 'Shenzen', 'Toro', 'VISAs', 'Ser', 'Cheareen', 'INTj'\n",
    "            , 'Linktank', 'Brentwood', \"Reports'\", 'HappyBalls', 'Kalata', 'Berend', 'Becu', 'Institue', 'Ecoli', 'Cabe', 'Microfinance', 'Hilal', 'Homaidan'\n",
    "            , 'CollegeClassifieds', 'Zensah', 'Enloe', 'BillMeLater', 'Kangan', 'Dti', 'Nj', 'Lipoma', 'Golfsmith','Atronic', 'Escows',  'Ent', 'Drs', 'Northstrom'\n",
    "            , 'Zales', 'CardsCapital', 'Blueline', 'Longaberger', 'Xceed', 'Wiki', 'Issaquah', 'AtriCure', 'Coolrail', 'Lumitip', 'Frigitronics', 'Cryo', 'Patt'\n",
    "            , \"Amex's\", 'CreditSecur', 'Emich', 'Acc', 'Amaroil', \"Fico's\", 'Biopharmaceutical', 'Bandera','Lindenwood', 'Sportage', 'Groc', 'Jus', 'Biomolecular'\n",
    "            , 'Bain', 'Tigran', 'Danielyan', 'Amberton', 'Statoliner', 'Ericson', 'Shillington', 'Papercutz', 'Bionicle', 'Reatin', 'Mouw', 'Abetterwatobuy'\n",
    "            , 'Abetterwaytobuy', 'Nikoli', \"Nikoli's\", 'Vo', 'Nieto', 'Tama', \"EMT's\", 'Fosgate', 'Accoustic', 'Morgantown', 'Clarksburg', 'Neo', 'QQitemZ'\n",
    "            , 'QQcmdZViewItemQQptZUS', 'Diaz', 'Collazo', 'Nicolau', 'Fortlauderdale', 'MesmoTV', 'ImproveTrafficNow', 'Primus', 'Jani', 'Legue', 'Flagg', 'Chaitanya'\n",
    "            , 'GoZone', 'Baltazar', 'Tencel', 'Bulkin', 'Temporomandibular', 'Townhome', 'Boynton', 'Fidelis', 'Mpls', 'DOWNOUR', 'Ibanez', 'McAllen', 'Kirksville', 'Relo'\n",
    "            , 'Benning', 'DQs', 'Msc', 'Hawkeye', 'Melrose', 'Hille', 'Bora', 'ServiceMagic', 'AdrenalinIsland', 'DjScottyd', 'Rebecca', 'Hassett', 'ThePlanet', 'Beltone'\n",
    "            , 'Ferotec', 'Raybestos', 'Manheim', 'Zayin', \"DQ's\", 'Spina', 'Bifida', 'Broadmoor', 'Equafax','Frid', 'AuroraLove', 'AnnaCFarney', 'Farney'\n",
    "            , 'ViewUserPage', 'Hillsborough', 'Graco', 'RogerFarney', 'Hotmail', 'Sentra', 'Euros','Darien', 'Pell', 'Cuny', 'Portuguez', 'McCombs', 'Komodo', 'Internet'\n",
    "            , 'Geico', 'Citifinancial', 'Netflix', 'Miata', 'Citigroup', 'Walmart']\n",
    "typ = typ[~typ['typo'].isin(entities)]\n",
    "\n",
    "#Add in those detected in Stanford NER\n",
    "entities.extend(orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = typ.loc[typ['typo'].isin([entity.lower() for entity in entities]), 'typo']\n",
    "a = typ.loc[typ['typo'].isin([acronym.lower() for acronym in acronyms]), 'typo']\n",
    "\n",
    "typ = typ[~typ['typo'].isin([entity.lower() for entity in entities])]\n",
    "typ = typ[~typ['typo'].isin([acronym.lower() for acronym in acronyms])]\n",
    "\n",
    "entities.extend(e)\n",
    "acronyms.extend(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-typos removed: 1795\n"
     ]
    }
   ],
   "source": [
    "# Payer is a word \n",
    "typ = typ[~typ['typo'].isin(['payor','payer', 'Payor', 'Payer'])]\n",
    "# Most US keyboards don't allow for accents, so ignore typo corrections with accents, e.g. fiancee and clientele\n",
    "typ = typ[typ['recommended'].str.contains('è') == False]\n",
    "typ = typ[typ['recommended'].str.contains('é') == False]\n",
    "typ = typ[~typ['typo'].isin(['Cafe', 'cafe', 'fiance', 'fiancee'])]\n",
    "# Words starting with or ending with ' were falsely listed as typo\n",
    "typ = typ[typ['typo'].str.contains(\"'\") == False]\n",
    "\n",
    "len_1 = typ.shape[0]\n",
    "diff = len_0 - len_1\n",
    "print(\"Non-typos removed: \" + str(diff))\n",
    "len_0 = len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangs = ['Yappy', 'Btw', 'Doggie', 'LOL', 'lol', 'PLS', 'WooHoo', 'Hmm', 'Aprox', 'Thx', 'Yay', 'Thanx', 'Woohoo', 'Flippin',  'HOOAH', 'WOOOOOO', 'OMG'\n",
    "          , 'SOOOO', 'Feh', 'WAAAY', 'Whoohoo',  'Lol', 'Wowww', 'Yessss', 'Goooo', 'Plz',  'Argh',  'XXXXX', 'XXXXXX',  'PLEEASSSEEE', 'Dawgs','Ahhhh']\n",
    "\n",
    "shorthands = ['Desc', 'Pmt',\"Gov't\", 'Pmts', 'Xwife', \"Int'l\", 'Intl', 'Pymt', 'Admin', 'admin', 'Pymts', 'pcm', 'FYI', 'bdrm']\n",
    "\n",
    "medical_terms = ['Osteomyelitis']\n",
    "\n",
    "typ = typ[~typ['typo'].isin(slangs)]\n",
    "typ = typ[~typ['typo'].isin(shorthands)]\n",
    "# Add lowercases\n",
    "slangs.extend(typ[typ['typo'].isin([word.lower() for word in slangs])]['typo'])\n",
    "shorthands.extend(typ[typ['typo'].isin([word.lower() for word in shorthands])]['typo'])\n",
    "typ = typ[~typ['typo'].isin([word.lower() for word in slangs])]\n",
    "typ = typ[~typ['typo'].isin([word.lower() for word in shorthands])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>author</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>yayeeyay</td>\n",
       "      <td>19</td>\n",
       "      <td>27.0</td>\n",
       "      <td>dc397b2f</td>\n",
       "      <td>affirmation; suggestion of encouragement, appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Janky</td>\n",
       "      <td>296</td>\n",
       "      <td>255.0</td>\n",
       "      <td>dc397b2f</td>\n",
       "      <td>Undesirable; less-than optimum.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.0</td>\n",
       "      <td>brutal</td>\n",
       "      <td>12</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40ece1ef</td>\n",
       "      <td>anything that makes you sweat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.0</td>\n",
       "      <td>skanky</td>\n",
       "      <td>9</td>\n",
       "      <td>48.0</td>\n",
       "      <td>485e4db7</td>\n",
       "      <td>Anything of or pertaining to a $10,000 hooker.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>hecka</td>\n",
       "      <td>8</td>\n",
       "      <td>18.0</td>\n",
       "      <td>b9dcf126</td>\n",
       "      <td>see synonyms at hella.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id      word up_votes down_votes    author  \\\n",
       "0      9.0  yayeeyay       19       27.0  dc397b2f   \n",
       "1      7.0     Janky      296      255.0  dc397b2f   \n",
       "2     13.0    brutal       12       45.0  40ece1ef   \n",
       "3     14.0    skanky        9       48.0  485e4db7   \n",
       "4     22.0     hecka        8       18.0  b9dcf126   \n",
       "\n",
       "                                          definition  \n",
       "0  affirmation; suggestion of encouragement, appr...  \n",
       "1                    Undesirable; less-than optimum.  \n",
       "2                      anything that makes you sweat  \n",
       "3     Anything of or pertaining to a $10,000 hooker.  \n",
       "4                             see synonyms at hella.  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Urban dictionary API does not work - HTTPError 429 (too many requests)\n",
    "# Source: https://github.com/bocong/urbandictionary-py/blob/master/urbandictionary.py\n",
    "# Fixed a bug (https://github.com/bocong/urbandictionary-py/issues/8)\n",
    "# Load manual csv file, found in Kaggle\n",
    "# Note: some definitions have commas and throw errors, but the full definition isn't needed so ignoring warning\n",
    "\n",
    "urbandict = pd.read_csv(\"urbandict-word-def.csv\", sep = \",\", error_bad_lines=False, warn_bad_lines=False)\n",
    "urbandict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>author</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1469027</th>\n",
       "      <td>4,064,666.0</td>\n",
       "      <td>dissapointed</td>\n",
       "      <td>89.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31c5f6a3</td>\n",
       "      <td>The wrong way to spell disappointed.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word_id          word up_votes down_votes    author  \\\n",
       "1469027 4,064,666.0  dissapointed     89.0       17.0  31c5f6a3   \n",
       "\n",
       "                                   definition  \n",
       "1469027  The wrong way to spell disappointed.  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urbandict.loc[urbandict['word'] == 'dissapointed'] \n",
    "#Urban dictionary has a variety of common typos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>author</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228618</th>\n",
       "      <td>562,818.0</td>\n",
       "      <td>ect</td>\n",
       "      <td>104.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>ca8d8385</td>\n",
       "      <td>electroconvulsive therapy. an electric current...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386595</th>\n",
       "      <td>1,029,419.0</td>\n",
       "      <td>ect</td>\n",
       "      <td>332.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>19b2b06</td>\n",
       "      <td>speaking of stupid people... its actually spel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507401</th>\n",
       "      <td>1,332,145.0</td>\n",
       "      <td>ect</td>\n",
       "      <td>114</td>\n",
       "      <td>49</td>\n",
       "      <td>1d65c1cd</td>\n",
       "      <td>bastardisation of 'etc.' or et cetera.;;;;only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698277</th>\n",
       "      <td>4,798,028.0</td>\n",
       "      <td>ect</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>ba9b2a72</td>\n",
       "      <td>ect is often made to look like a person with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582182</th>\n",
       "      <td>8,642,277.0</td>\n",
       "      <td>ect</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>262e4f73</td>\n",
       "      <td>ect. is an often falsely used abbreviation for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word_id word up_votes down_votes    author  \\\n",
       "228618    562,818.0  ect    104.0       50.0  ca8d8385   \n",
       "386595  1,029,419.0  ect    332.0      151.0   19b2b06   \n",
       "507401  1,332,145.0  ect      114         49  1d65c1cd   \n",
       "1698277 4,798,028.0  ect        5         51  ba9b2a72   \n",
       "2582182 8,642,277.0  ect        1          0  262e4f73   \n",
       "\n",
       "                                                definition  \n",
       "228618   electroconvulsive therapy. an electric current...  \n",
       "386595   speaking of stupid people... its actually spel...  \n",
       "507401   bastardisation of 'etc.' or et cetera.;;;;only...  \n",
       "1698277  ect is often made to look like a person with a...  \n",
       "2582182  ect. is an often falsely used abbreviation for...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urbandict.loc[urbandict['word'] == 'ect'] \n",
    "#Some words have multiple definitions, one of which is misspelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>author</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26794</th>\n",
       "      <td>65,778.0</td>\n",
       "      <td>youre</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>431d0462</td>\n",
       "      <td>New Zealand spelling of the word 'you're'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86221</th>\n",
       "      <td>197,651.0</td>\n",
       "      <td>youre</td>\n",
       "      <td>11</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3b24ff6d</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282345</th>\n",
       "      <td>741,050.0</td>\n",
       "      <td>youre</td>\n",
       "      <td>71.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>89bc4d7b</td>\n",
       "      <td>1) used interchangeably to mean either 'your' ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_id   word up_votes down_votes    author  \\\n",
       "26794   65,778.0  youre        8       34.0  431d0462   \n",
       "86221  197,651.0  youre       11       75.0  3b24ff6d   \n",
       "282345 741,050.0  youre     71.0       10.0  89bc4d7b   \n",
       "\n",
       "                                               definition  \n",
       "26794          New Zealand spelling of the word 'you're'.  \n",
       "86221                                                your  \n",
       "282345  1) used interchangeably to mean either 'your' ...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urbandict.loc[urbandict['word'] == 'youre'] \n",
    "# The ones with few upvotes are likely to be typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbandict['up_votes'] = pd.to_numeric(urbandict['up_votes'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbandict = urbandict.loc[urbandict['up_votes'] > 20] # Selected 20 - out of scope but would be interesting to see what the cut-off is for slang vs. error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbantypos = set(urbandict.loc[urbandict[\"definition\"].str.contains(\"spell|typo|illiterate\", na = False), 'word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanwords = set(urbandict['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanwords = urbanwords - urbantypos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typo</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>nbsp</td>\n",
       "      <td>tbsp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>nevermind</td>\n",
       "      <td>never mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>fo</td>\n",
       "      <td>few</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>brainer</td>\n",
       "      <td>briner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Hoorah</td>\n",
       "      <td>Howrah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          typo recommended\n",
       "39        nbsp        tbsp\n",
       "84   nevermind  never mind\n",
       "119         fo         few\n",
       "131    brainer      briner\n",
       "290     Hoorah      Howrah"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urban_typo = typ[typ['typo'].isin(urbanwords)]\n",
    "urban_typo = urban_typo[urban_typo['typo'].str.lower() != urban_typo['recommended'].str.lower()] # remove the ones where the only difference is the capitalisation, e.g. christmas, june\n",
    "urban_typo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slangs removed: 586\n"
     ]
    }
   ],
   "source": [
    "len(typ.loc[typ['typo'].isin(urbanwords), 'typo'])\n",
    "# Add these to slang\n",
    "slangs.extend(typ.loc[typ['typo'].isin(urbanwords), 'typo'])\n",
    "len_0 = typ.shape[0]\n",
    "typ = typ[~typ['typo'].isin(urbanwords)]\n",
    "len_1 = typ.shape[0]\n",
    "diff = len_0 - len_1\n",
    "print(\"Slangs removed: \" + str(diff))\n",
    "len_0 = len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4733/4733 [00:00<00:00, 94555.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# repeated letters for emphasis\n",
    "emphasis = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "patterns = [r\"so+$\", r\"wow+$\", r\"wa+y$\", r\"ye+s+$\", r\"ple+a+s+e+$\", r\"you+$\", r\"tha+nk+s*$\"]\n",
    "\n",
    "for i, typo in enumerate(tqdm(typ['typo'])):\n",
    "    for p in patterns:\n",
    "        pattern = re.compile(p)\n",
    "        if pattern.match(typo.lower()):\n",
    "            emphasis.append(typo)\n",
    "\n",
    "emphasis\n",
    "\n",
    "typ = typ[~typ['typo'].isin(emphasis)]\n",
    "slangs.extend(emphasis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique typos: 4723\n"
     ]
    }
   ],
   "source": [
    "# Some manual corrections of suggestions\n",
    "typ.loc[typ['typo'].isin(['im', 'Im', \"i'm\", 'IM']), \"recommended\"] = \"I'm\"\n",
    "typ.loc[typ['typo'].isin(['eg', 'Eg', 'EG']), \"recommended\"] = \"e.g.\"\n",
    "typ.loc[typ['typo'].isin(['takingmeplaces']), \"recommended\"] = \"taking me places\"\n",
    "\n",
    "print(\"Number of unique typos: \" + str(typ.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typo</th>\n",
       "      <th>recommended</th>\n",
       "      <th>drop</th>\n",
       "      <th>add</th>\n",
       "      <th>n_drop</th>\n",
       "      <th>n_add</th>\n",
       "      <th>n_changes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13670</th>\n",
       "      <td>siouxfallsamericandream</td>\n",
       "      <td>Americanization</td>\n",
       "      <td>[s, i, o, u, x, f, a, l, l, s, a, d, r, e, m]</td>\n",
       "      <td>[A, i, z, t, i, o, n]</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18450</th>\n",
       "      <td>bankofamerica</td>\n",
       "      <td>Americanization</td>\n",
       "      <td>[b, a, n, k, o, f, a]</td>\n",
       "      <td>[A, n, i, z, a, t, i, o, n]</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15531</th>\n",
       "      <td>musculoskeletal</td>\n",
       "      <td>musculature</td>\n",
       "      <td>[o, s, k, l, e, t, a, l]</td>\n",
       "      <td>[a, t, u, r]</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>thephotobuddy</td>\n",
       "      <td>telephotography</td>\n",
       "      <td>[h, b, u, d, d]</td>\n",
       "      <td>[e, l, g, r, a, p, h]</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Justfininshed</td>\n",
       "      <td>Justification</td>\n",
       "      <td>[i, n, s, h, e, d]</td>\n",
       "      <td>[i, c, a, t, i, o]</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          typo      recommended  \\\n",
       "13670  siouxfallsamericandream  Americanization   \n",
       "18450            bankofamerica  Americanization   \n",
       "15531          musculoskeletal      musculature   \n",
       "16997            thephotobuddy  telephotography   \n",
       "243              Justfininshed    Justification   \n",
       "\n",
       "                                                drop  \\\n",
       "13670  [s, i, o, u, x, f, a, l, l, s, a, d, r, e, m]   \n",
       "18450                          [b, a, n, k, o, f, a]   \n",
       "15531                       [o, s, k, l, e, t, a, l]   \n",
       "16997                                [h, b, u, d, d]   \n",
       "243                               [i, n, s, h, e, d]   \n",
       "\n",
       "                               add  n_drop  n_add  n_changes  \n",
       "13670        [A, i, z, t, i, o, n]      15      7         22  \n",
       "18450  [A, n, i, z, a, t, i, o, n]       7      9         16  \n",
       "15531                 [a, t, u, r]       8      4         12  \n",
       "16997        [e, l, g, r, a, p, h]       5      7         12  \n",
       "243             [i, c, a, t, i, o]       6      6         12  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the changes\n",
    "import difflib\n",
    "drop_list = []\n",
    "add_list = []\n",
    "n_drop = []\n",
    "n_add = []\n",
    "\n",
    "for i, row in typ.iterrows():  \n",
    "    typo = row['typo']\n",
    "    rec = row['recommended']\n",
    "    #print('{} => {}'.format(typo, rec))  \n",
    "    drop = []\n",
    "    add = []\n",
    "    if pd.isnull(rec):\n",
    "        add.append(\"\")\n",
    "        drop.append(\"\")\n",
    "    else:\n",
    "        for i,s in enumerate(difflib.ndiff(typo, rec)):\n",
    "            #print(s)\n",
    "            if s[0]==' ': continue\n",
    "            elif s[0]=='-':\n",
    "                drop.append(s[-1])\n",
    "            #    print(u'Delete \"{}\" from position {}'.format(s[-1],i))\n",
    "            elif s[0]=='+':\n",
    "                add.append(s[-1])\n",
    "            #    print(u'Add \"{}\" to position {}'.format(s[-1],i))    \n",
    "    drop_list.append(drop)\n",
    "    n_drop.append(len(drop))\n",
    "    add_list.append(add)\n",
    "    n_add.append(len(add))\n",
    "\n",
    "typ['drop'] = drop_list\n",
    "typ['add'] = add_list\n",
    "typ['n_drop'] = n_drop\n",
    "typ['n_add'] = n_add\n",
    "\n",
    "typ['n_changes'] = typ['n_drop'] + typ['n_add']\n",
    "typ = typ.sort_values(by=['n_changes'], ascending = False)\n",
    "typ.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, it was output to CSV, and I manually corrected any wrong typo correction recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typo</th>\n",
       "      <th>recommended</th>\n",
       "      <th>medical</th>\n",
       "      <th>not_typo</th>\n",
       "      <th>slang</th>\n",
       "      <th>entity</th>\n",
       "      <th>acronym</th>\n",
       "      <th>shorthand</th>\n",
       "      <th>html</th>\n",
       "      <th>fixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siouxfallsamericandream</td>\n",
       "      <td>Americanization</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Sioux Fallls American Dream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bankofamerica</td>\n",
       "      <td>Americanization</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>bank of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>musculoskeletal</td>\n",
       "      <td>musculature</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>musculoskeletal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thephotobuddy</td>\n",
       "      <td>telephotography</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>the photo buddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Justfininshed</td>\n",
       "      <td>Justification</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>just finished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      typo      recommended  medical  not_typo  slang  entity  \\\n",
       "0  siouxfallsamericandream  Americanization      nan       nan    nan     nan   \n",
       "1            bankofamerica  Americanization      nan       nan    nan     nan   \n",
       "2          musculoskeletal      musculature      nan       nan    nan     nan   \n",
       "3            thephotobuddy  telephotography      nan       nan    nan     nan   \n",
       "4            Justfininshed    Justification      1.0       1.0    nan     nan   \n",
       "\n",
       "   acronym  shorthand  html                        fixed  \n",
       "0      nan        nan   nan  Sioux Fallls American Dream  \n",
       "1      nan        nan   nan              bank of America  \n",
       "2      nan        nan   nan              musculoskeletal  \n",
       "3      nan        nan   nan              the photo buddy  \n",
       "4      nan        nan   nan                just finished  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typF = pd.read_csv(\"typos_changes_manual.csv\", encoding = \"ISO-8859-1\")\n",
    "typF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of medical terms: 64\n"
     ]
    }
   ],
   "source": [
    "medical = typF.loc[typF['medical'] == 1, 'typo']\n",
    "medical[:5]\n",
    "medical_terms.extend(medical)\n",
    "print(\"Number of medical terms: \" + str(len(medical_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new slangs: 29\n",
      "Number of total slangs: 664\n",
      "Entities removed: 113\n",
      "Acronyms removed: 10\n"
     ]
    }
   ],
   "source": [
    "# SLANGS\n",
    "s = typF.loc[typF['slang'] == 1, 'typo']\n",
    "slangs.extend(s)\n",
    "print(\"Number of new slangs: \" + str(len(s)))\n",
    "print(\"Number of total slangs: \" + str(len(slangs)))\n",
    "\n",
    "# ENTITIES\n",
    "len_0 = typF.shape[0]\n",
    "entities.extend(typF.loc[typF['entity'] == 1, 'typo'])\n",
    "typF = typF[typF['entity'] != 1]\n",
    "len_1 = typF.shape[0]\n",
    "diff = len_0 - len_1\n",
    "print(\"Entities removed: \" + str(diff))\n",
    "len_0 = len_1\n",
    "\n",
    "# ACRONYMS\n",
    "acronyms.extend(typF.loc[typF['acronym'] == 1, 'typo'])\n",
    "len_0 = typF.shape[0]\n",
    "typF = typF[typF['acronym'] != 1]\n",
    "len_1 = typF.shape[0]\n",
    "diff = len_0 - len_1\n",
    "print(\"Acronyms removed: \" + str(diff))\n",
    "len_0 = len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new shorthands: 62\n",
      "Number of total shorthands: 76\n"
     ]
    }
   ],
   "source": [
    "# From manual:\n",
    "shorthands = ['Desc', 'Pmt',\"Gov't\", 'Pmts', 'Xwife', \"Int'l\", 'Intl', 'Pymt', 'Admin', 'admin', 'Pymts', 'pcm', 'FYI', 'bdrm']\n",
    "s = typF.loc[typF['shorthand'] == 1, 'typo']\n",
    "shorthands.extend(s)\n",
    "print(\"Number of new shorthands: \" + str(len(s)))\n",
    "print(\"Number of total shorthands: \" + str(len(shorthands)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-typos: 136\n"
     ]
    }
   ],
   "source": [
    "print(\"non-typos: \" + str(len(typF.loc[typF['not_typo'] == 1, 'typo'])))\n",
    "# Remove non-typos - false positives\n",
    "typF = typF[typF['not_typo'] != 1]\n",
    "typF = typF[typF['shorthand'] != 1]\n",
    "typF = typF[typF['slang'] != 1]\n",
    "# There were some medical typos, so keeping them in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slangs: 664\n",
      "Shorthands: 76\n",
      "Acronyms: 645\n",
      "Entities: 1891\n",
      "Medical terms: 64\n",
      "Medical typos: 28\n",
      "Remaining typos: 4386\n"
     ]
    }
   ],
   "source": [
    "# Purposeful typos\n",
    "print(\"Slangs: \" + str(len(slangs)))\n",
    "print(\"Shorthands: \" + str(len(shorthands)))\n",
    "\n",
    "# Non-typos\n",
    "print(\"Acronyms: \" + str(len(acronyms)))\n",
    "print(\"Entities: \" + str(len(entities)))\n",
    "print(\"Medical terms: \" + str(len(medical_terms)))\n",
    "print(\"Medical typos: \" + str(len(typF.loc[typF['medical'] == 1, 'typo'])))\n",
    "\n",
    "print(\"Remaining typos: \" + str(len(typF['typo'])))\n",
    "typos = typF['typo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4386it [00:00, 6514.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Unintentional typos:\n",
    "# Use soundex to see if it's a phoenetic equivalent\n",
    "# Add a column for phonetic equivalent\n",
    "# See if typos are phonetically equivalent & get orthographic distance\n",
    "import soundex\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "phonetic_eq = []\n",
    "s = soundex.getInstance()\n",
    "\n",
    "for i, row in tqdm(typF.iterrows()):\n",
    "    typo = str(row['typo'])\n",
    "    corr = str(row['fixed'])\n",
    "    if s.soundex(typo) == s.soundex(corr):\n",
    "        phonetic_eq.append(1)\n",
    "    else:\n",
    "        phonetic_eq.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of phonetic equivalent: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7286821705426356"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percentage of phonetic equivalent: \")\n",
    "phonetic_eq.count(1)/len(typF['typo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4386it [00:00, 5179.50it/s]\n"
     ]
    }
   ],
   "source": [
    "typF['phonetic_eq'] = phonetic_eq\n",
    "\n",
    "# Orthographic distance:\n",
    "#import Levenshtein - getting import error\n",
    "# Source: https://medium.com/@yash_agarwal2/soundex-and-levenshtein-distance-in-python-8b4b56542e9e\n",
    "def get_levenshtein_distance(word1, word2):\n",
    "    \"\"\"\n",
    "    https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "    :param word1:\n",
    "    :param word2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word2 = word2.lower()\n",
    "    word1 = word1.lower()\n",
    "    matrix = [[0 for x in range(len(word2) + 1)] for x in range(len(word1) + 1)]\n",
    "\n",
    "    for x in range(len(word1) + 1):\n",
    "        matrix[x][0] = x\n",
    "    for y in range(len(word2) + 1):\n",
    "        matrix[0][y] = y\n",
    "\n",
    "    for x in range(1, len(word1) + 1):\n",
    "        for y in range(1, len(word2) + 1):\n",
    "            if word1[x - 1] == word2[y - 1]:\n",
    "                matrix[x][y] = min(\n",
    "                    matrix[x - 1][y] + 1,\n",
    "                    matrix[x - 1][y - 1],\n",
    "                    matrix[x][y - 1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix[x][y] = min(\n",
    "                    matrix[x - 1][y] + 1,\n",
    "                    matrix[x - 1][y - 1] + 1,\n",
    "                    matrix[x][y - 1] + 1\n",
    "                )\n",
    "\n",
    "    return matrix[len(word1)][len(word2)]\n",
    "\n",
    "levenshtein_dist = []\n",
    "\n",
    "for i, row in tqdm(typF.iterrows()):\n",
    "    typo = str(row['typo'])\n",
    "    corr = str(row['fixed'])\n",
    "    levenshtein_dist.append(get_levenshtein_distance(typo, corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "typF['levenshtein_dist'] = levenshtein_dist\n",
    "typF.index = typF['typo']\n",
    "typo_dict = dict(zip(typF['typo'], typF['fixed']))\n",
    "typo_dict['reqarding'] = 'rewarding' #one manual fix\n",
    "phon_dict = dict(zip(typF['typo'], typF['phonetic_eq']))\n",
    "lev_dict = dict(zip(typF['typo'], typF['levenshtein_dist']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Data source: https://github.com/glutanimate/wordlist-medicalterms-en\\n\\nwith open(\"medicalwordlist.txt\") as f:\\n    content = f.readlines()\\nofficial_medical_words = [x.strip() for x in content] \\nofficial_medical_words.extend(medical_terms)\\n\\nfrom tqdm import tqdm\\n\\nofficial_medical_words = set(official_medical_words)\\n\\nn_medical = []\\nn_slang = []\\nn_shorthands = []\\nn_typos = []\\nn_phonetic_eq = []\\navg_lev_dist = []\\ntotal_lev_dist = []\\nn_words = []\\nn_numbers = []\\n\\nclean_descs = []\\n\\ndef hasNumbers(inputString):\\n    return any(char.isdigit() for char in inputString)\\n\\n# Look for typos\\nfor i, desc in enumerate(tqdm(df[\\'desc_clean\\'])):\\n    # Initialize        \\n    n = 0\\n    desc_clean = \\'\\'\\n    med = 0\\n    slang = 0\\n    shorthand = 0\\n    typo = 0\\n    phonetic_eq = 0\\n    lev_avg = 0\\n    lev_total = 0\\n    words = 0\\n    num = 0\\n\\n    for word in desc.split(): \\n        if word in official_medical_words:\\n            med += 1\\n        if word in slangs:\\n            slang += 1\\n        if word in shorthands:\\n            shorthand += 1\\n        if hasNumbers(word):\\n            num += 1\\n        if word in typo_dict.keys():\\n            typo += 1\\n            desc_clean += typo_dict[word] # get the correction and add it to the clean text\\n            desc_clean += \" \"\\n            phonetic_eq += phon_dict[word] \\n            lev_total += lev_dict[word]\\n        else:\\n            desc_clean += word\\n            desc_clean += \" \"\\n                \\n    clean_descs.append(desc_clean.strip())\\n    n_numbers.append(num)\\n    n_typos.append(typo)\\n    n_medical.append(med)\\n    n_slang.append(slang)\\n    n_shorthands.append(shorthand)\\n    n_phonetic_eq.append(phonetic_eq)\\n    total_lev_dist.append(lev_total)\\n    n = len(desc_clean.strip().split())\\n    n_words.append(n)\\n    avg_lev_dist.append(lev_total/n)\\n    \\ndf[\\'n_words\\'] = n_words\\ndf[\\'tx_n_medical\\'] = n_medical\\ndf[\\'tx_n_slang\\'] = n_slang\\ndf[\\'tx_n_shorthands\\'] = n_shorthands\\ndf[\\'tx_n_typos\\'] = n_typos\\ndf[\\'tx_n_phonetic_eq\\'] = n_phonetic_eq\\ndf[\\'tx_avg_lev_dist\\'] = avg_lev_dist\\ndf[\\'tx_total_lev_dist\\'] = total_lev_dist\\ndf[\\'tx_n_numbers\\'] = n_numbers\\ndf[\\'desc_without_typo\\'] = clean_descs\\n\\ndf.head()\\n\\ndf.to_json(\"LoanStats_added_features.json\")'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Data source: https://github.com/glutanimate/wordlist-medicalterms-en\n",
    "\n",
    "with open(\"medicalwordlist.txt\") as f:\n",
    "    content = f.readlines()\n",
    "official_medical_words = [x.strip() for x in content] \n",
    "official_medical_words.extend(medical_terms)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "official_medical_words = set(official_medical_words)\n",
    "\n",
    "n_medical = []\n",
    "n_slang = []\n",
    "n_shorthands = []\n",
    "n_typos = []\n",
    "n_phonetic_eq = []\n",
    "avg_lev_dist = []\n",
    "total_lev_dist = []\n",
    "n_words = []\n",
    "n_numbers = []\n",
    "\n",
    "clean_descs = []\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "# Look for typos\n",
    "for i, desc in enumerate(tqdm(df['desc_clean'])):\n",
    "    # Initialize        \n",
    "    n = 0\n",
    "    desc_clean = ''\n",
    "    med = 0\n",
    "    slang = 0\n",
    "    shorthand = 0\n",
    "    typo = 0\n",
    "    phonetic_eq = 0\n",
    "    lev_avg = 0\n",
    "    lev_total = 0\n",
    "    words = 0\n",
    "    num = 0\n",
    "\n",
    "    for word in desc.split(): \n",
    "        if word in official_medical_words:\n",
    "            med += 1\n",
    "        if word in slangs:\n",
    "            slang += 1\n",
    "        if word in shorthands:\n",
    "            shorthand += 1\n",
    "        if hasNumbers(word):\n",
    "            num += 1\n",
    "        if word in typo_dict.keys():\n",
    "            typo += 1\n",
    "            desc_clean += typo_dict[word] # get the correction and add it to the clean text\n",
    "            desc_clean += \" \"\n",
    "            phonetic_eq += phon_dict[word] \n",
    "            lev_total += lev_dict[word]\n",
    "        else:\n",
    "            desc_clean += word\n",
    "            desc_clean += \" \"\n",
    "                \n",
    "    clean_descs.append(desc_clean.strip())\n",
    "    n_numbers.append(num)\n",
    "    n_typos.append(typo)\n",
    "    n_medical.append(med)\n",
    "    n_slang.append(slang)\n",
    "    n_shorthands.append(shorthand)\n",
    "    n_phonetic_eq.append(phonetic_eq)\n",
    "    total_lev_dist.append(lev_total)\n",
    "    n = len(desc_clean.strip().split())\n",
    "    n_words.append(n)\n",
    "    avg_lev_dist.append(lev_total/n)\n",
    "    \n",
    "df['n_words'] = n_words\n",
    "df['tx_n_medical'] = n_medical\n",
    "df['tx_n_slang'] = n_slang\n",
    "df['tx_n_shorthands'] = n_shorthands\n",
    "df['tx_n_typos'] = n_typos\n",
    "df['tx_n_phonetic_eq'] = n_phonetic_eq\n",
    "df['tx_avg_lev_dist'] = avg_lev_dist\n",
    "df['tx_total_lev_dist'] = total_lev_dist\n",
    "df['tx_n_numbers'] = n_numbers\n",
    "df['desc_without_typo'] = clean_descs\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.to_json(\"LoanStats_added_features.json\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'compound': 0.7003}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"LoanStats_added_features.json\")\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    #print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "    return score\n",
    "    \n",
    "sentiment_analyzer_scores(df['desc_without_typo'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Run once\\nfrom tqdm import tqdm\\n\\nneg_list = []\\nneu_list = []\\npos_list = []\\ncompound_list = []\\n\\n# Get sentiment scores\\nfor i, desc in enumerate(tqdm(df[\\'desc_without_typo\\'])):\\n    score = sentiment_analyzer_scores(desc)\\n    neg_list.append(score[\\'neg\\'])\\n    neu_list.append(score[\\'neu\\'])\\n    pos_list.append(score[\\'pos\\'])\\n    compound_list.append(score[\\'compound\\'])\\n    \\n\\ndf[\\'sentiment_neg\\'] = neg_list\\ndf[\\'sentiment_neu\\'] = neu_list\\ndf[\\'sentiment_pos\\'] = pos_list\\ndf[\\'sentiment_compound\\'] = compound_list\\n\\ndf.to_json(\"LoanStats_added_features.json\") # For future reading\\n\\ndf.head()\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Run once\n",
    "from tqdm import tqdm\n",
    "\n",
    "neg_list = []\n",
    "neu_list = []\n",
    "pos_list = []\n",
    "compound_list = []\n",
    "\n",
    "# Get sentiment scores\n",
    "for i, desc in enumerate(tqdm(df['desc_without_typo'])):\n",
    "    score = sentiment_analyzer_scores(desc)\n",
    "    neg_list.append(score['neg'])\n",
    "    neu_list.append(score['neu'])\n",
    "    pos_list.append(score['pos'])\n",
    "    compound_list.append(score['compound'])\n",
    "    \n",
    "\n",
    "df['sentiment_neg'] = neg_list\n",
    "df['sentiment_neu'] = neu_list\n",
    "df['sentiment_pos'] = pos_list\n",
    "df['sentiment_compound'] = compound_list\n",
    "\n",
    "df.to_json(\"LoanStats_added_features.json\") # For future reading\n",
    "\n",
    "df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addr_state</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_clean</th>\n",
       "      <th>desc_without_typo</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>has_words</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>id</th>\n",
       "      <th>len_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>tx_n_shorthands</th>\n",
       "      <th>tx_n_slang</th>\n",
       "      <th>tx_n_typos</th>\n",
       "      <th>tx_total_lev_dist</th>\n",
       "      <th>words_desc</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>sentiment_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZ</td>\n",
       "      <td>24,000.0</td>\n",
       "      <td>Borrower added on 12/22/11 &gt; I need to upgra...</td>\n",
       "      <td>I need to upgrade my business technologies.</td>\n",
       "      <td>I need to upgrade my business technologies.</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>860xx</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GA</td>\n",
       "      <td>30,000.0</td>\n",
       "      <td>Borrower added on 12/22/11 &gt; I plan to use t...</td>\n",
       "      <td>I plan to use this money to finance the motorc...</td>\n",
       "      <td>I plan to use this money to finance the motorc...</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1</td>\n",
       "      <td>519</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>106</td>\n",
       "      <td>309xx</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9430000000000001</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.7003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AZ</td>\n",
       "      <td>72,000.0</td>\n",
       "      <td>Borrower added on 12/15/11 &gt; I had recived a...</td>\n",
       "      <td>I had recived a loan from Citi Financial about...</td>\n",
       "      <td>I had received a loan from Citi Financial abou...</td>\n",
       "      <td>5 years</td>\n",
       "      <td>1</td>\n",
       "      <td>OWN</td>\n",
       "      <td>10</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>853xx</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8270000000000001</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>CA</td>\n",
       "      <td>25,000.0</td>\n",
       "      <td>Borrower added on 12/14/11 &gt; This loan will ...</td>\n",
       "      <td>This loan will consolidate a current personal ...</td>\n",
       "      <td>This loan will consolidate a current personal ...</td>\n",
       "      <td>2 years</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>100</td>\n",
       "      <td>192</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>902xx</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9380000000000001</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NY</td>\n",
       "      <td>35,000.0</td>\n",
       "      <td>Borrower added on 12/05/11 &gt; I plan to use t...</td>\n",
       "      <td>I plan to use these funds to better manage my ...</td>\n",
       "      <td>I plan to use these funds to better manage my ...</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>999</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>111xx</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.9412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     addr_state  annual_inc  \\\n",
       "0            AZ    24,000.0   \n",
       "1            GA    30,000.0   \n",
       "10           AZ    72,000.0   \n",
       "100          CA    25,000.0   \n",
       "1000         NY    35,000.0   \n",
       "\n",
       "                                                   desc  \\\n",
       "0       Borrower added on 12/22/11 > I need to upgra...   \n",
       "1       Borrower added on 12/22/11 > I plan to use t...   \n",
       "10      Borrower added on 12/15/11 > I had recived a...   \n",
       "100     Borrower added on 12/14/11 > This loan will ...   \n",
       "1000    Borrower added on 12/05/11 > I plan to use t...   \n",
       "\n",
       "                                             desc_clean  \\\n",
       "0           I need to upgrade my business technologies.   \n",
       "1     I plan to use this money to finance the motorc...   \n",
       "10    I had recived a loan from Citi Financial about...   \n",
       "100   This loan will consolidate a current personal ...   \n",
       "1000  I plan to use these funds to better manage my ...   \n",
       "\n",
       "                                      desc_without_typo emp_length  has_words  \\\n",
       "0           I need to upgrade my business technologies.  10+ years          1   \n",
       "1     I plan to use this money to finance the motorc...   < 1 year          1   \n",
       "10    I had received a loan from Citi Financial abou...    5 years          1   \n",
       "100   This loan will consolidate a current personal ...    2 years          1   \n",
       "1000  I plan to use these funds to better manage my ...   < 1 year          1   \n",
       "\n",
       "     home_ownership   id  len_desc  ...  tx_n_shorthands  tx_n_slang  \\\n",
       "0              RENT    0        43  ...                0           0   \n",
       "1              RENT    1       519  ...                0           0   \n",
       "10              OWN   10       220  ...                0           0   \n",
       "100            RENT  100       192  ...                0           0   \n",
       "1000           RENT  999       304  ...                0           0   \n",
       "\n",
       "      tx_n_typos tx_total_lev_dist  words_desc  zip_code  sentiment_neg  \\\n",
       "0              0                 0           7     860xx            0.0   \n",
       "1              3                 4         106     309xx            0.0   \n",
       "10             1                 1          43     853xx           0.05   \n",
       "100            1                 1          33     902xx            0.0   \n",
       "1000           0                 0          55     111xx           0.04   \n",
       "\n",
       "          sentiment_neu  sentiment_pos  sentiment_compound  \n",
       "0                   1.0            0.0                 0.0  \n",
       "1    0.9430000000000001          0.057              0.7003  \n",
       "10   0.8270000000000001          0.123              0.4767  \n",
       "100  0.9380000000000001          0.062                0.25  \n",
       "1000              0.647          0.312              0.9412  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"LoanStats_added_features.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No typos for:  88.13 %\n",
      "No slangs for:  97.95 %\n"
     ]
    }
   ],
   "source": [
    "# Finally, contrast the above features for defaulters vs. non-defaulters\n",
    "\n",
    "#- % typos / # words for defaulters vs. non-defaulters\n",
    "df['tx_perc_typo'] = df['tx_n_typos']/df['n_words']\n",
    "df['tx_perc_phonetic_eq'] = df['tx_n_phonetic_eq']/df['tx_n_typos']\n",
    "df['tx_n_typos' == 0, 'tx_perc_phonetic_eq'] = 1\n",
    "print(\"No typos for: \", round(df['tx_n_typos'].value_counts()[0]/df.shape[0]*100, 2), \"%\")\n",
    "print(\"No slangs for: \", round(df['tx_n_slang'].value_counts()[0]/df.shape[0]*100, 2), \"%\")\n",
    "\n",
    "keep = ['loan_status',                 'tx_n_medical',\n",
    "                         'tx_n_slang',              'tx_n_shorthands',\n",
    "                         'tx_n_typos',             'tx_n_phonetic_eq',\n",
    "                    'tx_avg_lev_dist',            'tx_total_lev_dist',\n",
    "                       'tx_n_numbers',            'desc_without_typo',\n",
    "                      'sentiment_neg',                'sentiment_neu',\n",
    "                      'sentiment_pos',           'sentiment_compound',\n",
    "                       'tx_perc_typo',          'tx_perc_phonetic_eq']\n",
    "df2 = df[keep]\n",
    "# Summarise income, loan amount by race\n",
    "by_default = df2.groupby([\"loan_status\"]).describe()\n",
    "by_default\n",
    "\n",
    "df['loan_status'] = df['loan_status'].map({1: 'Charged Off', 0: 'Fully Paid'})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #to hide warnings showing the directory name (for submission anonymity)\n",
    "\n",
    "# Summarise income, loan amount by race\n",
    "by_default = df.groupby([\"loan_status\"]).describe()\n",
    "by_default\n",
    "\n",
    "# keep the relevant statistics\n",
    "keep = [('sentiment_compound', 'count')\n",
    "        , ('sentiment_compound','mean'), ('sentiment_compound','std'), ('sentiment_compound','50%')\n",
    "        , ('sentiment_neg','mean'), ('sentiment_neg','std'), ('sentiment_neg','50%')\n",
    "        , ('sentiment_neu','mean'), ('sentiment_neu','std'), ('sentiment_neu','50%')\n",
    "        , ('sentiment_pos','mean'), ('sentiment_pos','std'), ('sentiment_pos','50%')]\n",
    "by_default = by_default[keep]\n",
    "\n",
    "# flatten multi-index file\n",
    "by_default.columns = [' '.join(col).strip() for col in by_default.columns.values]\n",
    "by_default = np.round(by_default, decimals=2)\n",
    "\n",
    "by_default.columns = ['N', 'Mean: Compound', 'Std dev: Compound', 'Median: Compound'\n",
    "                      ,'Mean: Negative', 'Std dev: Negative', 'Median: Negative'\n",
    "                     ,'Mean: Neutral', 'Std dev: Neutral', 'Median: Neutral'\n",
    "                     ,'Mean: Positive', 'Std dev: Positive', 'Median: Positive' ]\n",
    "\n",
    "by_default = np.round(by_default, decimals=2)\n",
    "by_default['N'] = by_default['N'].astype('int')\n",
    "by_default['N'] = by_default['N'].apply('{:,}'.format)\n",
    "\n",
    "temp = by_default.unstack().reset_index()\n",
    "temp.columns = ['Metrics', 'Loan Status', 'Value']\n",
    "temp = temp.pivot(index='Metrics', columns='Loan Status', values='Value')\n",
    "temp = temp.reindex(['N', 'Mean: Compound', 'Std dev: Compound', 'Median: Compound'\n",
    "                      ,'Mean: Negative', 'Std dev: Negative', 'Median: Negative'\n",
    "                     ,'Mean: Neutral', 'Std dev: Neutral', 'Median: Neutral'\n",
    "                     ,'Mean: Positive', 'Std dev: Positive', 'Median: Positive'])\n",
    "temp\n",
    "\n",
    "latex_output = temp.to_latex()\n",
    "with open(\"summary_sentiment.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Mean: Total Levenshtein Distance</th>\n",
       "      <th>Std dev: Total Levenshtein Distance</th>\n",
       "      <th>Median: Total Levenshtein Distance</th>\n",
       "      <th>Mean: Avg Levenshtein Distance</th>\n",
       "      <th>Std dev: Avg Levenshtein Distance</th>\n",
       "      <th>Median: Avg Levenshtein Distance</th>\n",
       "      <th>Mean: % Phonetic Equivalent</th>\n",
       "      <th>Std dev: % Phonetic Equivalent</th>\n",
       "      <th>Median: % Phonetic Equivalent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Charged Off</th>\n",
       "      <td>6,403</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fully Paid</th>\n",
       "      <td>35,906</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  N  Mean: Total Levenshtein Distance  \\\n",
       "loan_status                                             \n",
       "Charged Off   6,403                              0.27   \n",
       "Fully Paid   35,906                               0.2   \n",
       "\n",
       "             Std dev: Total Levenshtein Distance  \\\n",
       "loan_status                                        \n",
       "Charged Off                                 0.94   \n",
       "Fully Paid                                  0.72   \n",
       "\n",
       "             Median: Total Levenshtein Distance  \\\n",
       "loan_status                                       \n",
       "Charged Off                                 0.0   \n",
       "Fully Paid                                  0.0   \n",
       "\n",
       "             Mean: Avg Levenshtein Distance  \\\n",
       "loan_status                                   \n",
       "Charged Off                            0.01   \n",
       "Fully Paid                              0.0   \n",
       "\n",
       "             Std dev: Avg Levenshtein Distance  \\\n",
       "loan_status                                      \n",
       "Charged Off                               0.02   \n",
       "Fully Paid                                0.03   \n",
       "\n",
       "             Median: Avg Levenshtein Distance  Mean: % Phonetic Equivalent  \\\n",
       "loan_status                                                                  \n",
       "Charged Off                               0.0                         0.77   \n",
       "Fully Paid                                0.0                         0.78   \n",
       "\n",
       "             Std dev: % Phonetic Equivalent  Median: % Phonetic Equivalent  \n",
       "loan_status                                                                 \n",
       "Charged Off                            0.39                            1.0  \n",
       "Fully Paid                             0.38                            1.0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #to hide warnings showing the directory name (for submission anonymity)\n",
    "\n",
    "by_default = df.groupby([\"loan_status\"]).describe()\n",
    "by_default\n",
    "\n",
    "#'tx_avg_lev_dist', 'tx_n_medical', 'tx_n_numbers', 'tx_n_phonetic_eq'\n",
    "#, 'tx_n_shorthands', 'tx_n_slang', 'tx_n_typos', 'tx_perc_phonetic_eq', 'tx_perc_typo', 'tx_total_lev_dist'\n",
    "\n",
    "# keep the relevant statistics\n",
    "keep = [('n_words', 'count')\n",
    "        , ('tx_total_lev_dist', 'mean'), ('tx_total_lev_dist', 'std'),('tx_total_lev_dist', '50%')\n",
    "        , ('tx_avg_lev_dist','mean'), ('tx_avg_lev_dist','std'), ('tx_avg_lev_dist','50%')\n",
    "        , ('tx_perc_phonetic_eq','mean'), ('tx_perc_phonetic_eq','std'), ('tx_perc_phonetic_eq','50%')]\n",
    "by_default = by_default[keep]\n",
    "\n",
    "# flatten multi-index file\n",
    "by_default.columns = [' '.join(col).strip() for col in by_default.columns.values]\n",
    "by_default = np.round(by_default, decimals=2)\n",
    "\n",
    "by_default.columns = ['N', 'Mean: Total Levenshtein Distance', 'Std dev: Total Levenshtein Distance', 'Median: Total Levenshtein Distance'\n",
    "                      , 'Mean: Avg Levenshtein Distance', 'Std dev: Avg Levenshtein Distance', 'Median: Avg Levenshtein Distance'\n",
    "                     ,'Mean: % Phonetic Equivalent', 'Std dev: % Phonetic Equivalent', 'Median: % Phonetic Equivalent']\n",
    "\n",
    "by_default = np.round(by_default, decimals=2)\n",
    "by_default['N'] = by_default['N'].astype('int')\n",
    "by_default['N'] = by_default['N'].apply('{:,}'.format)\n",
    "\n",
    "by_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = by_default.unstack().reset_index()\n",
    "temp.columns = ['Metrics', 'Loan Status', 'Value']\n",
    "temp = temp.pivot(index='Metrics', columns='Loan Status', values='Value')\n",
    "temp = temp.reindex(['N', 'Mean: Total Levenshtein Distance', 'Std dev: Total Levenshtein Distance', 'Median: Total Levenshtein Distance'\n",
    "                      , 'Mean: Avg Levenshtein Distance', 'Std dev: Avg Levenshtein Distance', 'Median: Avg Levenshtein Distance'\n",
    "                     ,'Mean: % Phonetic Equivalent', 'Std dev: % Phonetic Equivalent', 'Median: % Phonetic Equivalent'])\n",
    "\n",
    "latex_output = temp.to_latex()\n",
    "with open(\"summary_distance.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Mean: # Shorthands</th>\n",
       "      <th>Std dev: # Shorthands</th>\n",
       "      <th>Median: # Shorthands</th>\n",
       "      <th>Mean: # Typos</th>\n",
       "      <th>Std dev: # Typos</th>\n",
       "      <th>Median: # Typos</th>\n",
       "      <th>Mean: # Slang</th>\n",
       "      <th>Std dev: # Slang</th>\n",
       "      <th>Median: # Slang</th>\n",
       "      <th>Mean: # Medical</th>\n",
       "      <th>Std dev: # Medical</th>\n",
       "      <th>Median: # Medical</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Charged Off</th>\n",
       "      <td>6,403</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>15.61</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fully Paid</th>\n",
       "      <td>35,906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.41</td>\n",
       "      <td>14.11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  N  Mean: # Shorthands  Std dev: # Shorthands  \\\n",
       "loan_status                                                      \n",
       "Charged Off   6,403                0.01                   0.09   \n",
       "Fully Paid   35,906                 0.0                   0.08   \n",
       "\n",
       "             Median: # Shorthands  Mean: # Typos  Std dev: # Typos  \\\n",
       "loan_status                                                          \n",
       "Charged Off                   0.0           0.24              0.77   \n",
       "Fully Paid                    0.0           0.17              0.59   \n",
       "\n",
       "             Median: # Typos  Mean: # Slang  Std dev: # Slang  \\\n",
       "loan_status                                                     \n",
       "Charged Off              0.0           0.03              0.23   \n",
       "Fully Paid               0.0           0.02              0.18   \n",
       "\n",
       "             Median: # Slang  Mean: # Medical  Std dev: # Medical  \\\n",
       "loan_status                                                         \n",
       "Charged Off              0.0              9.8               15.61   \n",
       "Fully Paid               0.0             9.41               14.11   \n",
       "\n",
       "             Median: # Medical  \n",
       "loan_status                     \n",
       "Charged Off                4.0  \n",
       "Fully Paid                 4.0  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #to hide warnings showing the directory name (for submission anonymity)\n",
    "\n",
    "by_default = df.groupby([\"loan_status\"]).describe()\n",
    "by_default\n",
    "\n",
    "#'tx_avg_lev_dist', 'tx_n_medical', 'tx_n_numbers', 'tx_n_phonetic_eq'\n",
    "#, 'tx_n_shorthands', 'tx_n_slang', 'tx_n_typos', 'tx_perc_phonetic_eq', 'tx_perc_typo', 'tx_total_lev_dist'\n",
    "\n",
    "# keep the relevant statistics\n",
    "keep = [('n_words', 'count')\n",
    "        , ('tx_n_shorthands', 'mean'), ('tx_n_shorthands', 'std'),('tx_n_shorthands', '50%')\n",
    "        , ('tx_n_typos','mean'), ('tx_n_typos','std'), ('tx_n_typos','50%')\n",
    "        , ('tx_n_slang','mean'), ('tx_n_slang','std'), ('tx_n_slang','50%')\n",
    "        , ('tx_n_medical','mean'), ('tx_n_medical','std'), ('tx_n_medical','50%')]\n",
    "by_default = by_default[keep]\n",
    "\n",
    "# flatten multi-index file\n",
    "by_default.columns = [' '.join(col).strip() for col in by_default.columns.values]\n",
    "by_default = np.round(by_default, decimals=2)\n",
    "\n",
    "by_default.columns = ['N', 'Mean: # Shorthands', 'Std dev: # Shorthands', 'Median: # Shorthands'\n",
    "                      , 'Mean: # Typos', 'Std dev: # Typos', 'Median: # Typos'\n",
    "                     ,'Mean: # Slang', 'Std dev: # Slang', 'Median: # Slang'\n",
    "                     ,'Mean: # Medical', 'Std dev: # Medical', 'Median: # Medical']\n",
    "\n",
    "by_default = np.round(by_default, decimals=2)\n",
    "by_default['N'] = by_default['N'].astype('int')\n",
    "by_default['N'] = by_default['N'].apply('{:,}'.format)\n",
    "\n",
    "by_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = by_default.unstack().reset_index()\n",
    "temp.columns = ['Metrics', 'Loan Status', 'Value']\n",
    "temp = temp.pivot(index='Metrics', columns='Loan Status', values='Value')\n",
    "temp = temp.reindex(['N', 'Mean: # Shorthands', 'Std dev: # Shorthands', 'Median: # Shorthands'\n",
    "                      , 'Mean: # Typos', 'Std dev: # Typos', 'Median: # Typos'\n",
    "                     ,'Mean: # Slang', 'Std dev: # Slang', 'Median: # Slang'\n",
    "                     ,'Mean: # Medical', 'Std dev: # Medical', 'Median: # Medical'])\n",
    "temp\n",
    "\n",
    "latex_output = temp.to_latex()\n",
    "with open(\"summary_typos.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predict default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'] = df['loan_status'].map({'Charged Off':1, 'Fully Paid':0})\n",
    "# Split into features and labels\n",
    "y, X = dmatrices(\"loan_status ~ loan_amnt + annual_inc\", data=df, return_type='dataframe') #+ C(home_ownership,Treatment(reference='RENT')) + C(purpose,Treatment(reference='debt_consolidation'))\n",
    "X.columns = [\"Intercept\", \"Loan amount\", \"Annual income\"] #\"Home ownership: mortgage\", \"Home ownership: none\", \"Home ownership: other\",\"Home ownership: own\", \"Purpose: car\", \"Purpose: credit card\", \"Purpose: educational\", \"Purpose: home improvement\", \"Purpose: house\", \"Purpose: major purchase\", \"Purpose: medical\", \"Purpose: moving\", \"Purpose: other\", \"Purpose: renewable energy\", \"Purpose: small business\", \"Purpose: vacation\", \"Purpose: wedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Loan amount</th>\n",
       "      <th>Annual income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8221817721079037</td>\n",
       "      <td>-0.7035464817182224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.1592795879754982</td>\n",
       "      <td>-0.610041950052741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.619923082587347</td>\n",
       "      <td>0.044489771605629266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6603748204914583</td>\n",
       "      <td>-0.6879623931073089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39137036501543665</td>\n",
       "      <td>-0.5321215069981731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8221817721079037</td>\n",
       "      <td>0.683437404653086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42301</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5228385132037985</td>\n",
       "      <td>-0.31238585758429166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42302</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0109565489937566</td>\n",
       "      <td>0.9795350882604439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5262094913624744</td>\n",
       "      <td>-0.5539392310534521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42304</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48238677529968715</td>\n",
       "      <td>-0.6412101272745682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42305 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Intercept         Loan amount        Annual income\n",
       "0            0.0 -0.8221817721079037  -0.7035464817182224\n",
       "1            0.0 -1.1592795879754982   -0.610041950052741\n",
       "2            0.0  -0.619923082587347 0.044489771605629266\n",
       "3            0.0 -0.6603748204914583  -0.6879623931073089\n",
       "4            0.0 0.39137036501543665  -0.5321215069981731\n",
       "...          ...                 ...                  ...\n",
       "42300        0.0 -0.8221817721079037    0.683437404653086\n",
       "42301        0.0  0.5228385132037985 -0.31238585758429166\n",
       "42302        0.0 -1.0109565489937566   0.9795350882604439\n",
       "42303        0.0  0.5262094913624744  -0.5539392310534521\n",
       "42304        0.0 0.48238677529968715  -0.6412101272745682\n",
       "\n",
       "[42305 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_X = pd.DataFrame(StandardScaler().fit_transform(X), columns = [\"Intercept\", \"Loan amount\", \"Annual income\"])\n",
    "scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      "Intercept                          0.0\n",
      "Loan amount     -7.591597707268732e-16\n",
      "Annual income   1.9124283766341812e-16\n",
      "dtype: float64\n",
      "Std: \n",
      "Intercept                      0.0\n",
      "Loan amount     1.0000118191434066\n",
      "Annual income   1.0000118191435166\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \")\n",
    "print(scaled_X.mean())\n",
    "\n",
    "print(\"Std: \")\n",
    "print(scaled_X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      "Intercept                      1.0\n",
      "Loan amount     11,097.50147736674\n",
      "Annual income   69,145.17975889376\n",
      "dtype: float64\n",
      "Std: \n",
      "Intercept                       0.0\n",
      "Loan amount      7,416.332679060225\n",
      "Annual income   64,168.771373845775\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \")\n",
    "print(X.mean())\n",
    "print(\"Std: \")\n",
    "print(X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.421451\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.008</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>    <td>loan_status</td>         <td>AIC:</td>        <td>35664.9833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-22 14:37</td>       <td>BIC:</td>        <td>35690.9413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>42305</td>       <td>Log-Likelihood:</td>    <td>-17829.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>2</td>            <td>LL-Null:</td>        <td>-17982.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>42302</td>        <td>LLR p-value:</td>    <td>7.7995e-67</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>         <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>     <td>-1.6734</td>  <td>0.0297</td>  <td>-56.3143</td> <td>0.0000</td> <td>-1.7316</td> <td>-1.6151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Loan amount</th>   <td>0.0000</td>   <td>0.0000</td>   <td>14.3088</td> <td>0.0000</td> <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Annual income</th> <td>-0.0000</td>  <td>0.0000</td>  <td>-13.5691</td> <td>0.0000</td> <td>-0.0000</td> <td>-0.0000</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                         Results: Logit\n",
       "=================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.008     \n",
       "Dependent Variable: loan_status      AIC:              35664.9833\n",
       "Date:               2020-04-22 14:37 BIC:              35690.9413\n",
       "No. Observations:   42305            Log-Likelihood:   -17829.   \n",
       "Df Model:           2                LL-Null:          -17982.   \n",
       "Df Residuals:       42302            LLR p-value:      7.7995e-67\n",
       "Converged:          1.0000           Scale:            1.0000    \n",
       "No. Iterations:     6.0000                                       \n",
       "-----------------------------------------------------------------\n",
       "                  Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
       "-----------------------------------------------------------------\n",
       "Intercept        -1.6734   0.0297 -56.3143 0.0000 -1.7316 -1.6151\n",
       "Loan amount       0.0000   0.0000  14.3088 0.0000  0.0000  0.0000\n",
       "Annual income    -0.0000   0.0000 -13.5691 0.0000 -0.0000 -0.0000\n",
       "=================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the summary\n",
    "logit = sm.Logit(y, X)\n",
    "results = logit.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scaled_X.reindex(y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingDataError",
     "evalue": "exog contains inf or nans",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDataError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-99986728c6c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# View the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBinaryModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         if (not issubclass(self.__class__, MultinomialModel) and\n\u001b[1;32m    436\u001b[0m                 not np.all((self.endog >= 0) & (self.endog <= 1))):\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDiscreteModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_on_perfect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 77\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 672\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_handle_constant\u001b[0;34m(self, hasconst)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mexog_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mMissingDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exog contains inf or nans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mexog_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog_max\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexog_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingDataError\u001b[0m: exog contains inf or nans"
     ]
    }
   ],
   "source": [
    "# View the summary\n",
    "logit = sm.Logit(np.array(y), np.array(scaled_X))\n",
    "results = logit.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>n_words</th>\n",
       "      <th>tx_n_shorthands</th>\n",
       "      <th>tx_n_slang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5,000.0</td>\n",
       "      <td>24,000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2,500.0</td>\n",
       "      <td>30,000.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6,500.0</td>\n",
       "      <td>72,000.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6,200.0</td>\n",
       "      <td>25,000.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>14,000.0</td>\n",
       "      <td>35,000.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intercept  loan_amnt  annual_inc  n_words  tx_n_shorthands  tx_n_slang\n",
       "0           1.0    5,000.0    24,000.0      7.0              0.0         0.0\n",
       "1           1.0    2,500.0    30,000.0    108.0              0.0         0.0\n",
       "10          1.0    6,500.0    72,000.0     43.0              0.0         0.0\n",
       "100         1.0    6,200.0    25,000.0     33.0              0.0         0.0\n",
       "1000        1.0   14,000.0    35,000.0     55.0              0.0         0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "# Split into features and labels\n",
    "\n",
    "y, X = dmatrices(\"loan_status ~ loan_amnt + annual_inc + n_words + tx_n_shorthands + tx_n_slang\", data=df, return_type='dataframe')\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.421260\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.009</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>    <td>loan_status</td>         <td>AIC:</td>        <td>35654.7853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-18 17:31</td>       <td>BIC:</td>        <td>35706.7013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>42305</td>       <td>Log-Likelihood:</td>    <td>-17821.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>            <td>LL-Null:</td>        <td>-17982.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>42299</td>        <td>LLR p-value:</td>    <td>3.6527e-67</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>-1.6812</td>  <td>0.0307</td>  <td>-54.7230</td> <td>0.0000</td> <td>-1.7414</td> <td>-1.6210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Loan amount</th>     <td>0.0000</td>   <td>0.0000</td>   <td>14.2440</td> <td>0.0000</td> <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Annual income</th>   <td>-0.0000</td>  <td>0.0000</td>  <td>-13.5540</td> <td>0.0000</td> <td>-0.0000</td> <td>-0.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Number of words</th> <td>-0.0000</td>  <td>0.0002</td>   <td>-0.0294</td> <td>0.9765</td> <td>-0.0004</td> <td>0.0004</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th># Shorthands</th>    <td>0.2408</td>   <td>0.1435</td>   <td>1.6780</td>  <td>0.0934</td> <td>-0.0405</td> <td>0.5221</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th># Slang</th>         <td>0.2307</td>   <td>0.0625</td>   <td>3.6880</td>  <td>0.0002</td> <td>0.1081</td>  <td>0.3532</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                         Results: Logit\n",
       "=================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.009     \n",
       "Dependent Variable: loan_status      AIC:              35654.7853\n",
       "Date:               2020-04-18 17:31 BIC:              35706.7013\n",
       "No. Observations:   42305            Log-Likelihood:   -17821.   \n",
       "Df Model:           5                LL-Null:          -17982.   \n",
       "Df Residuals:       42299            LLR p-value:      3.6527e-67\n",
       "Converged:          1.0000           Scale:            1.0000    \n",
       "No. Iterations:     6.0000                                       \n",
       "-----------------------------------------------------------------\n",
       "                  Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
       "-----------------------------------------------------------------\n",
       "Intercept        -1.6812   0.0307 -54.7230 0.0000 -1.7414 -1.6210\n",
       "Loan amount       0.0000   0.0000  14.2440 0.0000  0.0000  0.0000\n",
       "Annual income    -0.0000   0.0000 -13.5540 0.0000 -0.0000 -0.0000\n",
       "Number of words  -0.0000   0.0002  -0.0294 0.9765 -0.0004  0.0004\n",
       "# Shorthands      0.2408   0.1435   1.6780 0.0934 -0.0405  0.5221\n",
       "# Slang           0.2307   0.0625   3.6880 0.0002  0.1081  0.3532\n",
       "=================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns = [\"Intercept\", \"Loan amount\", \"Annual income\", \"Number of words\", \"# Shorthands\", \"# Slang\"]\n",
    "# View the summary\n",
    "logit = sm.Logit(y, X)\n",
    "results = logit.fit()\n",
    "results.summary2()\n",
    "\n",
    "latex_output = results.summary2().as_latex()\n",
    "with open(\"01_model_purposeful_typo_Logit_LaTeX.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median number of words: 22.0\n",
      "If you have median income/loan/description length with no shorthand or slang, your probability of default is:\n",
      "14.95 %\n",
      "If you have median income/loan/description length and have a shorthand, your probability of default is:\n",
      "18.27 %\n",
      "If you have median income/loan/description length and have a slang, your probability of default is:\n",
      "18.12 %\n"
     ]
    }
   ],
   "source": [
    "from math import e\n",
    "median_income = df['annual_inc'].median()\n",
    "median_loan = df['loan_amnt'].median()\n",
    "median_n_words = df['n_words'].median()\n",
    "print(\"Median number of words: \" + str(median_n_words))\n",
    "\n",
    "# Function to return the probability, computes e^x/(1+e^x)\n",
    "coefs = results.params.values\n",
    "coefs\n",
    "\n",
    "print(\"If you have median income/loan/description length with no shorthand or slang, your probability of default is:\")\n",
    "e_value = e**(coefs[0]+(coefs[1]*median_loan)+(coefs[2]*median_income)+(coefs[3]*median_n_words))\n",
    "print(np.round(e_value/(1+e_value)*100, 2), \"%\")\n",
    "\n",
    "print(\"If you have median income/loan/description length and have a shorthand, your probability of default is:\")\n",
    "e_value = e**(coefs[0]+(coefs[1]*median_loan)+(coefs[2]*median_income)+(coefs[3]*median_n_words)+(coefs[4]))\n",
    "print(np.round(e_value/(1+e_value)*100, 2), \"%\")\n",
    "\n",
    "print(\"If you have median income/loan/description length and have a slang, your probability of default is:\")\n",
    "e_value = e**(coefs[0]+(coefs[1]*median_loan)+(coefs[2]*median_income)+(coefs[3]*median_n_words)+(coefs[5]))\n",
    "print(np.round(e_value/(1+e_value)*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
